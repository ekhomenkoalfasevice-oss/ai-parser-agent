[
  {
    "title": "huggingface_hub v1.0: Five Years of Building the Foundation of Open Machine Learning",
    "link": "https://huggingface.co/blog/huggingface-hub-v1",
    "published_at": "2025-10-27T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Go! (2024-2025) Measuring Growth and Impact Building for the Next Decade Modern HTTP Infrastructure with httpx and hf_xet Agents Made Simple with MCP and Tiny-Agents A Fully-Featured CLI for Modern Workflows Cleaning House for the Future The Migration Guide Acknowledgments TL;DR: After five years of development, huggingface_hub has reached v1.0 - a milestone that marks the library's maturity as the Python package powering 200,000 dependent libraries and providing core functionality for accessing over 2 million public models, 0.5 million public datasets, and 1 million public Spaces. Go! (2024-2025) The Great Shift: Git to HTTP (2022) An Expanding API Surface (2022‚Äì2024) Building for the Next Decade Modern HTTP Infrastructure with httpx and hf_xet Agents Made Simple with MCP and Tiny-Agents A Fully-Featured CLI for Modern Workflows Cleaning House for the Future The Migration Guide Modern HTTP Infrastructure with httpx and hf_xet Agents Made Simple with MCP and Tiny-Agents A Fully-Featured CLI for Modern Workflows Cleaning House for the Future üöÄ We highly recommend upgrading to v1.0 to benefit from major performance improvements and new capabilities. The streamlined hf command replaces the legacy huggingface-cli with a modern resource-action pattern: hf auth login for authentication hf download and hf upload for file transfers hf repo for repository management hf cache ls and hf cache rm for cache management hf jobs run for cloud compute The CLI comes with a sandboxed installer , making it easy to upgrade without breaking existing dev environments: With autocompletion support and an installer that works across platforms, the CLI now feels as polished as any modern developer tool.",
    "full_text": "The Story Behind the Library The Foundation Years (2020-2021) The Great Shift: Git to HTTP (2022) An Expanding API Surface (2022‚Äì2024) Ready. Xet. Go! (2024-2025) Measuring Growth and Impact Building for the Next Decade Modern HTTP Infrastructure with httpx and hf_xet Agents Made Simple with MCP and Tiny-Agents A Fully-Featured CLI for Modern Workflows Cleaning House for the Future The Migration Guide Acknowledgments TL;DR: After five years of development, huggingface_hub has reached v1.0 - a milestone that marks the library's maturity as the Python package powering 200,000 dependent libraries and providing core functionality for accessing over 2 million public models, 0.5 million public datasets, and 1 million public Spaces. This release introduces breaking changes designed to support the next decade of open machine learning, driven by a global community of almost 300 contributors and millions of users.\n\nThe Story Behind the Library The Foundation Years (2020-2021) The Great Shift: Git to HTTP (2022) An Expanding API Surface (2022‚Äì2024) Ready. Xet. Go! (2024-2025)\n\nThe Great Shift: Git to HTTP (2022)\n\nAn Expanding API Surface (2022‚Äì2024)\n\nBuilding for the Next Decade Modern HTTP Infrastructure with httpx and hf_xet Agents Made Simple with MCP and Tiny-Agents A Fully-Featured CLI for Modern Workflows Cleaning House for the Future The Migration Guide\n\nModern HTTP Infrastructure with httpx and hf_xet\n\nAgents Made Simple with MCP and Tiny-Agents\n\nA Fully-Featured CLI for Modern Workflows\n\nCleaning House for the Future\n\nüöÄ We highly recommend upgrading to v1.0 to benefit from major performance improvements and new capabilities.\n\nMajor changes in this release include the migration to httpx as the backend library, a completely redesigned hf CLI (which replaces the deprecated huggingface-cli ) featuring a Typer-based interface with a significantly expanded feature set, and full adoption of hf_xet for file transfers, replacing the legacy hf_transfer . You can find the full release notes here .\n\nWe‚Äôve worked hard to ensure that huggingface_hub v1.0.0 remains backward compatible. In practice, most ML libraries should work seamlessly with both v0.x and v1.x versions. The main exception is transformers , which explicitly requires huggingface_hub v0.x in its v4 releases and v1.x in its upcoming v5 release. For a detailed compatibility overview across libraries, refer to the table in this issue .\n\nEvery major library has a story. For huggingface_hub , it began with a simple idea: what if sharing machine learning models could be as easy as sharing code on GitHub?\n\nIn the early days of the Hugging Face Hub, researchers and practitioners faced a common frustration. Training a state-of-the-art model required significant compute resources and expertise. Once trained, these models often lived in isolation, stored on local machines and shared via (broken) Google Drive links. The AI community was duplicating work, wasting resources, and missing opportunities for collaboration.\n\nThe Hugging Face Hub emerged as the answer to this challenge. Initially, it was primarily used to share checkpoints compatible with the transformers library. All the Python code for interacting with the Hub lived within this library, making it inaccessible for other libraries to reuse.\n\nIn late 2020, we shipped huggingface_hub v0.0.1 with a simple mission: extract the internal logic from transformers and create a dedicated library that would unify how to access and share machine learning models and datasets on the Hugging Face Hub. Initially, the library was as straightforward as a Git wrapper for downloading files and managing repositories. Five years and 35+ releases later, huggingface_hub has evolved far beyond its origins.\n\nThe early releases established the basics. Version 0.0.8 introduced our first APIs, wrapping Git commands to interact with repositories. Version 0.0.17 brought token-based authentication, enabling secure access to private repositories and uploads. These were humble beginnings, but they laid the groundwork for everything that followed.\n\nIn June 2022, version 0.8.1 marked a pivotal moment: we introduced the HTTP Commit API. Instead of requiring Git and Git LFS installations, users could now upload files directly through HTTP requests. The new create_commit() API simplified workflows dramatically, especially for large model files that are cumbersome to use with Git LFS. In addition, a git-aware cache file layout was introduced. All libraries (not only transformers, but third party ones as well) would now share the same cache, with explicit versioning and file deduplication.\n\nThis wasn't just a technical improvement. It was a philosophical shift. We were no longer building a Git wrapper for transformers; we were building purpose-built infrastructure for machine learning artifacts that could power any library in the ML ecosystem.\n\nAs the Hub grew from a model repository into a full platform, huggingface_hub kept pace with an expanding API surface. Core repository primitives matured: listing trees, browsing refs and commits, reading files or syncing folders, managing tags, branches, and release cycles. Repository metadata and webhooks rounded up the offering so teams could react to changes in real time.\n\nIn parallel, Spaces emerged as a simple yet powerful way to host and share interactive ML demos directly on the Hub. Over time, huggingface_hub gained full programmatic control to deploy and manage Spaces (hardware requests, secrets, environment configuration, uploads). To deploy models on production-scale infrastructure, Inference Endpoints were integrated as well. Finally, the Jobs API came later (Q3 2025) to complete our compute offering.\n\nThe social and community layers became first-class citizens too: from APIs for pull requests and comments , to user and organization info, repository likes, following and followers, all the way through Collections to curate and share sets of related resources. Everyday ergonomics improved too: seamless authentication in Colab, resumable downloads, reliable uploads of large-scale folders, and more.\n\nThen came version 0.28.0 and the Inference Providers ecosystem. Instead of a single inference backend, we partnered with multiple serverless providers (Together AI, SambaNova, Replicate, Cerebras, Groq, and more) to serve one API with transparent routing. We adopted a pay-per-request inference architecture that matched how people actually wanted to work.\n\nVersion 0.30.0 introduced Xet, a groundbreaking new protocol for storing large objects in Git repositories. Unlike Git LFS, which deduplicates at the file level, Xet operates at the chunk level (64KB chunks). When you update a large file in a dataset or a model, only the changed chunks are uploaded or downloaded, not the entire file.\n\nThe migration was massive , starting with 20 petabytes across over 500,000 repositories. Yet it happened transparently, with full backward compatibility. One year later, all 77PB+ over 6,000,000 repositories have been migrated to the Xet backend, allowing for much faster (and smarter!) uploads and downloads. This happened with no user intervention, and no disruption to existing workflows üî•\n\nMeasuring the growth and impact of an open-source library is a tricky task. Numbers tell a story of their own:\n\n113.5 million monthly downloads , 1.6 billion total (October 2025).\n\nPowers access to 2M+ public models, 500k+ public datasets, 1M+ public Spaces, and about twice as much when accounting for private repos.\n\nUsed by 60k+ users daily, 550k+ monthly\n\nTrusted by 200k+ companies from startups to Fortune 500\n\nBut the real scale becomes clear when you look at the ecosystem. huggingface_hub is a dependency for over 200,000 repositories on GitHub and 3,000 packages on PyPI, powering everything from major third-party frameworks like Keras, LangChain, PaddleOCR, ChatTTS, YOLO, Google Generative AI, Moshi, NVIDIA NeMo, and Open Sora, to countless smaller libraries and tools across the ML landscape. Our own ecosystem (transformers, diffusers, datasets, sentence-transformers, lighteval, gradio, peft, trl, smolagents, timm, lerobot, etc.) benefits from this foundation as well.\n\nThe remarkable part? Most of the third-party integrations happened organically, and we played no role in them. The Hugging Face Hub empowers the ML community in countless ways, yet we're continually humbled by how far it has gone and how widely it's used.\n\nVersion 1.0 isn't just about reaching a milestone. It's about building the foundation for the next decade of open machine learning . The breaking changes we've made aren't arbitrary; they're strategic decisions that position huggingface_hub to scale with the explosive growth of AI while maintaining the reliability that millions of developers depend on.\n\nThe most significant architectural change in v1.0 is our migration from requests to httpx . This isn't just dependency churn. It's a fundamental upgrade that brings the library into the modern era of HTTP.\n\nWhy httpx? The benefits are substantial: native HTTP/2 support for better connection efficiency and true thread safety that enables safe connection reuse across multiple threads. Most importantly, httpx provides a unified API for both synchronous and asynchronous operations, eliminating the subtle behavioral differences that existed between our sync and async inference clients.\n\nThe migration was designed to be as transparent as possible. Most users won't need to change anything. For those with custom HTTP backends, we've provided clear migration paths from configure_http_backend() to set_client_factory() and set_async_client_factory() .\n\nAdditionally, hf_xet is now the default package for uploading and downloading files to and from the Hub, replacing the previously optional hf_transfer , which has now been fully removed.\n\nVersion 0.32.0 introduced Model Context Protocol (MCP) integration and tiny-agents , fundamentally changing how developers build AI agents. What once required complex framework integration now takes approximately 70 lines of Python.\n\nThe MCPClient provides a standardized way for AI agents to interact with tools, while the tiny-agents CLI lets you run agents directly from the Hub. Connect to local or remote MCP servers, use any Gradio Space as a tool, and build conversational agents that feel natural and responsive.\n\nAll of this is built on top of our existing InferenceClient and the dozens of Inference Providers it supports. We do believe Agents are the future, and huggingface_hub is there to provide the building blocks that enable AI builders to play with them.\n\nThe CLI has evolved from a simple command-line tool into a comprehensive interface for ML operations . The streamlined hf command replaces the legacy huggingface-cli with a modern resource-action pattern:\n\nhf auth login for authentication\n\nhf download and hf upload for file transfers\n\nhf repo for repository management\n\nhf cache ls and hf cache rm for cache management\n\nhf jobs run for cloud compute\n\nThe CLI comes with a sandboxed installer , making it easy to upgrade without breaking existing dev environments:\n\nWith autocompletion support and an installer that works across platforms, the CLI now feels as polished as any modern developer tool.\n\nVersion 1.0 removes legacy patterns that were holding us back. The Git-based Repository class is gone. HTTP-based methods like upload_file() and create_commit() are simpler, more reliable, and better suited for modern workflows. The HfFolder token management has been replaced with explicit login() , logout() , and get_token() functions. The old InferenceApi class has been superseded by the more feature-complete InferenceClient . hf_transfer has been fully replaced by hf_xet binary package.\n\nThese changes weren't made lightly. Most deprecations were announced months in advance with clear warnings and migration guidance. The result is a cleaner, more maintainable codebase that can focus on forward-looking features rather than supporting deprecated patterns.\n\nWe understand that breaking changes are disruptive. That's why we've invested heavily in making the migration as smooth as possible. Our comprehensive migration guide provides step-by-step instructions for every change with explanations of why each change was necessary.\n\nMost importantly, we've maintained backward compatibility wherever possible. HfHubHttpError , for example, inherits from both the old requests and new httpx base HTTPError classes, ensuring that error handling continues to work across versions.\nWith this release, we're fully committing to the future and we will focus exclusively on v1.0 and beyond, ensuring we can deliver the performance, features, and tools the community needs to interact with the Hugging Face Hub. Previous v0.* versions will remain available on PyPI, but they will only receive vulnerability updates.\n\nWe‚Äôve worked hard to ensure that huggingface_hub v1.0.0 remains backward compatible. In practice, most ML libraries should work seamlessly with both v0.x and v1.x versions. The main exception is transformers , which explicitly requires huggingface_hub v0.x in its v4 releases and v1.x in its upcoming v5 release. For a detailed compatibility overview across libraries, refer to the table in this issue .\n\nTo our 280+ contributors who built this library through code, documentation, translations, and community support, thank you!\n\nWe‚Äôre also deeply grateful to the entire Hugging Face community for their feedback, bug reports, and suggestions that have shaped this library.\n\nFinally, a huge thank you to our users -from individual developers to large enterprises- for trusting huggingface_hub to power your workflows. Your support drives us to keep improving and innovating.\n\nPlease star us on GitHub ‚≠ê to show your support and help us continue building the foundation of open machine learning. It has been five years, but it is still only the beginning!\n\nMore Articles from our Blog\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Hugging Face and VirusTotal collaborate to strengthen AI security",
    "link": "https://huggingface.co/blog/virustotal",
    "published_at": "2025-10-22T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Why this matters How the collaboration works Benefits for the community Join us We‚Äôre excited to announce a new collaboration between Hugging Face and VirusTotal , the world‚Äôs leading threat-intelligence and malware analysis platform. This collaboration enhances the security of files shared across the Hugging Face Hub, helping protect the machine learning community from malicious or compromised assets. Threats can take many forms: Malicious payloads disguised as model files or archives Files that have been compromised before upload Binary assets linked to known malware campaigns Dependencies or serialized objects that execute unsafe code when loaded By collaborating with VirusTotal, we‚Äôre adding an extra layer of protection and visibility by enabling files shared through Hugging Face to be checked against one of the largest and most trusted malware intelligence databases in the world.",
    "full_text": "Why this matters How the collaboration works Benefits for the community Join us We‚Äôre excited to announce a new collaboration between Hugging Face and VirusTotal , the world‚Äôs leading threat-intelligence and malware analysis platform.\nThis collaboration enhances the security of files shared across the Hugging Face Hub, helping protect the machine learning community from malicious or compromised assets.\n\nTL;DR - Starting today, every one of the 2.2M+ public model and datasets repositories on the Hugging Face Hub is being continuously scanned with VirusTotal.\n\nAI models are powerful but they‚Äôre also complex digital artifacts that can include large binary files, serialized data, and dependencies that sometimes carry hidden risks.\nAs of today HF Hub hosts 2.2 Million Public model artifacts. As we continue to grow into the world‚Äôs largest open platform for Machine Learning models and datasets, ensuring that shared assets remain safe is essential.\n\nThreats can take many forms:\n\nMalicious payloads disguised as model files or archives\n\nFiles that have been compromised before upload\n\nBinary assets linked to known malware campaigns\n\nDependencies or serialized objects that execute unsafe code when loaded\n\nBy collaborating with VirusTotal, we‚Äôre adding an extra layer of protection and visibility by enabling files shared through Hugging Face to be checked against one of the largest and most trusted malware intelligence databases in the world.\n\nWhenever you visit a repository page or a file or directory page, the Hub will automatically retrieve VirusTotal information about the corresponding files. Example\n\nWe compare the file hash against VirusTotal‚Äôs threat-intelligence database.\n\nIf a file hash has been previously analyzed by VirusTotal, its status (clean or malicious) is retrieved.\n\nNo raw file contents are shared with VirusTotal maintaining user privacy and compliance with Hugging Face‚Äôs data protection principles.\n\nResults include metadata such as detection counts, known-bad relationships, or associated threat-campaign intelligence where relevant.\n\nThis provides valuable context to users and organizations before they download or integrate files from the Hub.\n\nTransparency: Users can see if files have been previously flagged or analyzed in VirusTotal‚Äôs ecosystem.\n\nSafety: Organizations can integrate VirusTotal checks into their CI/CD or deployment workflows to help prevent the spread of malicious assets.\n\nEfficiency: Leveraging existing VirusTotal intelligence reduces the need for repeated or redundant scanning.\n\nTrust: Together, we‚Äôre making the Hugging Face Hub a more secure, reliable place to collaborate on open-source AI.\n\nIf you‚Äôd like to learn more about this integration or explore ways to contribute to a safer open-source AI ecosystem, reach out to security@huggingface.co .\n\nTogether, we can make AI collaboration not just open but secure by design.\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Unlock the power of images with AI Sheets",
    "link": "https://huggingface.co/blog/aisheets-unlock-images",
    "published_at": "2025-10-21T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Your images have stories to tell Generate and transform text and images in the same flow Step-by-step guide Upload your data Understanding AI actions Extract text from images. Describe and categorize images - Generate captions for product photos, classify document types, or tag images by content Extract structured data - Pull line items from receipts, data from charts, or text from scanned documents Add context and metadata - Automatically label images with relevant attributes, quality scores, or custom annotations Just like text columns, you can iterate on prompts, manually edit outputs, and use thumbs-up to teach the model what you want. What you can do: Generate images from text - Create social media graphics, thumbnails, or illustrations that match your content Edit and transform existing images - Modify uploaded images or generated visuals‚Äîchange styles, add elements, adjust compositions Create variations at scale - Generate multiple versions or styles to test what resonates with your audience Build visual content libraries - Produce consistent branded assets across large content campaigns Example: Creating a content calendar with visuals Imagine you're planning a month of social media posts about healthy recipes.",
    "full_text": "Your images have stories to tell Generate and transform text and images in the same flow Step-by-step guide Upload your data Understanding AI actions Extract text from images. Clean, transform, and enrich text Edit and transform images. Export your dataset What's next? üß≠ TL;DR : Hugging Face AI Sheets is an open-source tool for supercharging datasets with AI models , no code required. Now with vision support : extract data from images (receipts, documents), generate visuals from text, and edit images‚Äîall in a spreadsheet. Powered by thousands of open models via Inference Providers.\n\nYour images have stories to tell\n\nGenerate and transform text and images in the same flow\n\nStep-by-step guide Upload your data Understanding AI actions Extract text from images. Clean, transform, and enrich text Edit and transform images. Export your dataset\n\nClean, transform, and enrich text\n\nWe are excited to release a massive update to Hugging Face AI Sheets , the open-source tool for building, transforming, and enriching data with open AI models. AI Sheets leverages Inference Providers , which means you can use thousands of open models powered by the best inference providers on the planet.\n\nThe first version of AI Sheets made structuring and enriching textual content a breeze. Now, we're adding vision to AI Sheets.\n\nImages are everywhere‚Äîproduct photos, receipts, screenshots, diagrams, charts, logos. These documents contain structured information waiting to be extracted, analyzed, and transformed. Today, you can finally work with visual content directly in AI Sheets: view images, analyze them, extract information, generate new ones, and even edit them in real-time ‚Äîall in the same workflow.\n\nImages contain valuable information‚Äîproduct catalogs, support tickets, research archives, receipts, documents. Now you can upload images directly or use datasets with images, and use vision models to extract, analyze, and structure the information inside them.\n\nDescribe and categorize images - Generate captions for product photos, classify document types, or tag images by content\n\nExtract structured data - Pull line items from receipts, data from charts, or text from scanned documents\n\nAdd context and metadata - Automatically label images with relevant attributes, quality scores, or custom annotations\n\nJust like text columns, you can iterate on prompts, manually edit outputs, and use thumbs-up to teach the model what you want. Your feedback becomes few-shot examples for better results.\n\nExample: From receipts to structured expenses\n\nImagine you're back from a trip with a stack of receipts. Upload them to AI Sheets and create a column with a prompt like: Extract the merchant name, date, total amount, and expense category from this receipt\n\nAI Sheets processes each receipt and gives you a clean table with all the details extracted. You can edit any mistakes, validate good results with thumbs-up, and regenerate to improve the rest. Export the final dataset as CSV or Parquet for your expense tracking tool.\n\nOr maybe you're digitizing handwritten recipes from old family notebooks. Create columns to extract ingredients, cooking time, and cuisine type‚Äîturning your personal archive into a searchable, structured dataset.\n\nNeed visuals for your content? AI Sheets can generate and edit images directly in your spreadsheet using AI models, keeping your entire content creation workflow in one place. What you can do:\n\nGenerate images from text - Create social media graphics, thumbnails, or illustrations that match your content\n\nEdit and transform existing images - Modify uploaded images or generated visuals‚Äîchange styles, add elements, adjust compositions\n\nCreate variations at scale - Generate multiple versions or styles to test what resonates with your audience\n\nBuild visual content libraries - Produce consistent branded assets across large content campaigns\n\nExample: Creating a content calendar with visuals Imagine you're planning a month of social media posts about healthy recipes. You have a spreadsheet with post titles and descriptions, but no images yet.\n\nCreate an image column with a prompt like: Generate an appetizing food photo for: {{title}}. Style: bright, overhead shot, natural lighting.\n\nAI Sheets generates a unique image for each post. Not quite right? Create another column to edit them: Transform the image to have a rustic wooden background and add fresh herbs as garnish.\n\nYou can iterate on generation and editing prompts and try different approaches. Your entire content calendar‚Äîcopy and visuals‚Äîlives in one spreadsheet, ready to schedule or export.\n\nNow let‚Äôs see AI Sheets in action. We will use open models to unlock the knowledge within handwritten recipes like the ones you could find from your grandma.\n\nWe have a folder with photos that we can simply upload to the app.\n\nThe result is a spreadsheet like this:\n\nEach column in your spreadsheet can be transformed, extracted from, queried, and anything you can imagine using AI actions.\n\nTo see this in action, click on the overlay on top of any column:\n\nImage columns come with image operations like extracting text, asking the image, object detection, colorization, adding text, and any custom action you can think of.\n\nText columns include summarization, keyword extraction, translation, and custom actions.\n\nA prompt and a model define every AI action. Let‚Äôs see what we can do with our handwritten recipes dataset!\n\nAI Sheets comes with a template to extract text from images:\n\nThe result of this action is an AI-generated column with the transcribed text. Let‚Äôs see an example:\n\nFor the above image, the extracted text is as follows:\n\nNot bad! But we see it has included printed text for the header and footer, and we‚Äôre interested in the recipe text. The reason this text is included is that we have used the default template for text extraction, which is as follows:\n\nExtract and transcribe all visible text from the image, including signs, labels, documents, or any written content\n\nLet‚Äôs now try a custom prompt.\n\nHere is the extracted recipe details:\n\n- 1 box Duncan Hines Yellow Cake Mix - 1 box instant lemon pudding - 2/3 cups water - 1/2 cup Mazola oil - 4 eggs - Lemon flavoring to taste - Put in mixing bowl and beat for 10 minutes\n\nThis is great! But what about more complex images? By default, AI Sheets uses models with a good balance of speed and accuracy, but you can experiment with thousands of models. The above example uses the default vision language model Qwen/Qwen2.5-VL-7B-Instruct .\n\nLet‚Äôs test a SoTA reasoning model, Qwen/Qwen3-VL-235B-A22B-Reasoning , with a more challenging image.\n\nHere‚Äôs the comparison between the models:\n\nBoth models produce very similar outputs, but with two subtle but important details ( in bold ): the temperature and a key ingredient: spinach.\n\nOnce we are satisfied with the extracted text, we can further transform and enrich it. We need to perform an AI action with the new column as follows:\n\nWe now have a beautifully structured HTML page for each recipe:\n\nFinally, AI Sheets integrates image-to-image models like Qwen-Image-Edit. This means you can run AI actions to transform and enrich your images.\n\nFor example, let‚Äôs say you want to give your recipes and old-looking style, you need to go to the column and use the B&W template like so:\n\nOnce you're happy with your new dataset, export it to the Hub! You can export it to an organization, your personal profile or make it private if you don't want to share it with the community.\n\nYou can check out the dataset we have just created.\n\nYou can try AI Sheets without installing or downloading and deploying it locally from the GitHub repo . To run locally and get the most out of it, we recommend you subscribe to PRO and get 20x monthly inference usage.\n\nIf you have questions or suggestions, let us know in the Community tab or by opening an issue on GitHub .\n\nMore Articles from our Blog\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Smol2Operator: Post-Training GUI Agents for Computer Use",
    "link": "https://huggingface.co/blog/smol2operator",
    "published_at": "2025-09-23T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Data Transformation and Unified Action Space The Challenge of Inconsistent Action Spaces Our Unified Approach Example Data Transformation (Bonus) Custom Action Space Adaptation with Action Space Converter Key Features Usage Example Transformed and Released Datasets The Challenge of Inconsistent Action Spaces (Bonus) Custom Action Space Adaptation with Action Space Converter 2. This process highlighted the significant inconsistencies in function signatures across different datasets and allowed us to: Remove undesired or redundant actions Standardize parameter naming conventions Create a cohesive action vocabulary Remove undesired or redundant actions Create a cohesive action vocabulary (Bonus) Flexible Adaptation Framework : Our transformation pipeline includes utilities that allow users to: Adapt the entire dataset to their own action space naming conventions using the utils/action_space_converter.py tool Extract and analyze the current action space structure Adapt the entire dataset to their own action space naming conventions using the utils/action_space_converter.py tool Extract and analyze the current action space structure Here are real examples from our action conversion system ( preprocessing/action_conversion.py ) showing how we transform heterogeneous action representations into our unified format (grounding coordinates normalized to [0,1]): Before (Original Action Dataset Formats): After (Unified Action Dataset Formats): This unification process was essential for creating coherent training data that allows the model to learn consistent action patterns across diverse GUI environments. You can use this tool to transform one action signature (function names, parameter names, and parameter value changes, ...) into another: The Action Space Converter provides: Configurable Mappings : Define custom mappings between unified actions and your preferred action names Parameter Transformation : Rename parameters, apply value transformations, and set default values Flexible Architecture : Support for both simple parameter mappings and complex custom transformation functions Validation : Built-in validation to ensure mapping configurations are valid This tool enables researchers and practitioners to: Customize Training Data : Adapt the dataset to match their specific action vocabulary requirements Domain Adaptation : Transform actions for different platforms (mobile vs. desktop vs. web) Framework Integration : Easily align training data with existing automation frameworks Rapid Experimentation : Quickly test different action space configurations Release Preparation : Standardize action spaces for production deployment with consistent naming conventions The Action Space Converter is particularly valuable for preparing datasets for training, as it ensures consistent action vocabularies across different deployment environments while maintaining compatibility with existing automation frameworks.",
    "full_text": "Table of Contents Introduction 1. Data Transformation and Unified Action Space The Challenge of Inconsistent Action Spaces Our Unified Approach Example Data Transformation (Bonus) Custom Action Space Adaptation with Action Space Converter Key Features Usage Example Transformed and Released Datasets 2. Phase 1: From Zero to Perception Training Data Optimization Experiments Image Resolution and Coordinate System Analysis Key Findings Phase 1 Results 3. Phase 2: From Perception to Cognition Training Data Phase 2 Results 4. All you need is Open Source 5. Conclusion What's Next? TL;DR: This work shows how a lightweight vision‚Äìlanguage model can acquire GUI-grounded skills and evolve into an agentic GUI coder. We release all training recipes, data-processing tools, resulting model, demo and datasets to enable full reproducibility and foster further research ü´°. Find the collection here .\n\n1. Data Transformation and Unified Action Space The Challenge of Inconsistent Action Spaces Our Unified Approach Example Data Transformation (Bonus) Custom Action Space Adaptation with Action Space Converter Key Features Usage Example Transformed and Released Datasets\n\nThe Challenge of Inconsistent Action Spaces\n\n(Bonus) Custom Action Space Adaptation with Action Space Converter\n\n2. Phase 1: From Zero to Perception Training Data Optimization Experiments Image Resolution and Coordinate System Analysis Key Findings Phase 1 Results\n\nImage Resolution and Coordinate System Analysis\n\n3. Phase 2: From Perception to Cognition Training Data Phase 2 Results\n\n4. All you need is Open Source\n\n1. Data Transformation and Unified Action Space The Challenge of Inconsistent Action Spaces Our Unified Approach Example Data Transformation Custom Action Space Adaptation with Action Space Converter Key Features Usage Example Transformed and Released Datasets\n\nThe Challenge of Inconsistent Action Spaces\n\nCustom Action Space Adaptation with Action Space Converter\n\n2. Phase 1: From Zero to Perception Training Data Optimization Experiments Image Resolution and Coordinate System Analysis Key Findings Phase 1 Results\n\nImage Resolution and Coordinate System Analysis\n\n3. Phase 2: From Perception to Cognition Training Data Phase 2 Results\n\n4. All you need is Open Source\n\nGraphical User Interface (GUI) automation is one of the most challenging frontiers in computer vision. Developing models that see and interact with user interfaces enables AI agents to navigate mobile, desktop, and web platforms. This will reshape the future of digital interaction.\n\nIn this blog post, we present a comprehensive approach to training vision-language models for GUI automation through a multi-phase training strategy. We demonstrate how to transform a model with zero grounding capabilities into an agentic coder capable of understanding and interacting with graphical interfaces.\n\nRather than aiming for a SOTA model, our goal is to demonstrate the entire process, from data processing to model training, and, in doing so, show how to unlock GUI-grounding capabilities in VLMs.\n\nGUI capabilities combine understanding of the interface and precise element localization. These abilities enable the model to translate high-level tasks into low-level GUI actions such as clicking, typing, ‚Ä¶\n\nOur approach leverages SmolVLM2-2.2B-Instruct as the baseline model, a small powerful vision-language model that initially has no grounding capabilities for GUI tasks. This makes it an ideal candidate to demonstrate the effectiveness of our training methodology. Through our two-phase training process, we first instill grounding capabilities in the model, then enhance it with agentic reasoning abilities using Supervised Fine-Tuning (SFT).\n\nWe evaluate our approach on an established perception benchmark: ScreenSpot-v2 , which tests the model‚Äôs ability to understand and locate elements within screenshots. Our process is inspired by the AGUVIS paper, and we leverage their carefully curated datasets to build upon their foundational work.\n\nEvolution of ScreenSpot-v2 performance during the training phase of the base model SmolVLM2-2.2B-Instruct .\n\nThis section explains how we convert heterogeneous GUI actions format from multiple datasets into a single unified format . By standardizing function names, signatures, and parameters, we create consistent, high-quality data that forms the foundation for effective model training.\n\nOne of the primary challenges when working with multiple GUI automation datasets is the lack of standardization in action representations. Different datasets use varying function signatures, parameter naming conventions, and action taxonomies, making it difficult to train a unified model across diverse data sources.\n\nWe took the open-source datasets ( xlangai/aguvis-stage1 , xlangai/aguvis-stage2 ), originally used by AGUVIS , and implemented a comprehensive data transformation pipeline to create a unified action space. Our approach involved:\n\nFunction Parsing and Normalization : We developed a function parser (see utils/function_parser.py ) that can extract and parse function calls from various formats across all datasets. This parser supports any function signature format, handles complex parameter structures, and can reconstruct function calls with proper parameter ordering.\n\nAction Space Unification : We implemented a comprehensive action conversion system (see preprocessing/action_conversion.py ) that transforms all original action representations into a standardized function naming and argument structure. This process highlighted the significant inconsistencies in function signatures across different datasets and allowed us to: Remove undesired or redundant actions Standardize parameter naming conventions Create a cohesive action vocabulary\n\nRemove undesired or redundant actions\n\nCreate a cohesive action vocabulary\n\n(Bonus) Flexible Adaptation Framework : Our transformation pipeline includes utilities that allow users to: Adapt the entire dataset to their own action space naming conventions using the utils/action_space_converter.py tool Extract and analyze the current action space structure\n\nAdapt the entire dataset to their own action space naming conventions using the utils/action_space_converter.py tool\n\nExtract and analyze the current action space structure\n\nHere are real examples from our action conversion system ( preprocessing/action_conversion.py ) showing how we transform heterogeneous action representations into our unified format (grounding coordinates normalized to [0,1]):\n\nBefore (Original Action Dataset Formats):\n\nAfter (Unified Action Dataset Formats):\n\nThis unification process was essential for creating coherent training data that allows the model to learn consistent action patterns across diverse GUI environments.\n\nTo maximize flexibility for different use cases, we developed the Action Space Converter ( utils/action_space_converter.py ), a tool that allows users to easily adapt from an action space to their own custom action vocabularies and naming conventions.\n\nYou can use this tool to transform one action signature (function names, parameter names, and parameter value changes, ...) into another:\n\nThe Action Space Converter provides:\n\nConfigurable Mappings : Define custom mappings between unified actions and your preferred action names\n\nParameter Transformation : Rename parameters, apply value transformations, and set default values\n\nFlexible Architecture : Support for both simple parameter mappings and complex custom transformation functions\n\nValidation : Built-in validation to ensure mapping configurations are valid\n\nThis tool enables researchers and practitioners to:\n\nCustomize Training Data : Adapt the dataset to match their specific action vocabulary requirements\n\nDomain Adaptation : Transform actions for different platforms (mobile vs.¬†desktop vs.¬†web)\n\nFramework Integration : Easily align training data with existing automation frameworks\n\nRapid Experimentation : Quickly test different action space configurations\n\nRelease Preparation : Standardize action spaces for production deployment with consistent naming conventions\n\nThe Action Space Converter is particularly valuable for preparing datasets for training, as it ensures consistent action vocabularies across different deployment environments while maintaining compatibility with existing automation frameworks.\n\nThrough this pipeline, we transform the open-source datasets xlangai/aguvis-stage1 , xlangai/aguvis-stage2 into our unified action space (see here ). The output of this process is released as two new fully formatted datasets: smolagents/aguvis-stage-1 and smolagents/aguvis-stage-2 .\n\nPhase 1 leverages the smolagents/aguvis-stage-1 dataset, which introduces GUI grounding by pairing low-level instructions with diverse executable actions (expressed in code form). For example, a user/assistant turn in smolagents/aguvis-stage-1 follows the structure:\n\nEach sample links a screenshot with multi-turn user/assistant interactions, enabling the model to learn fine-grained action grounding across dialogue turns. During fine-tuning, the data collator masks everything except the assistant‚Äôs answers when computing the loss.\n\nBefore proceeding with full-scale Phase 1 training, we conducted comprehensive ablation studies to determine optimal training configurations\n\nWe experimented with different image sizes and coordinate representation systems to identify the optimal configuration for SmolVLM2:\n\nImage Sizes Tested : 384px, 768px, 1152px\n\nCoordinate Systems : Pixel coordinates vs.¬†normalized coordinates (0-1 range)\n\nTraining Data : 400K samples from Aguvis datasets\n\nSome SOTA GUI VLMs (e.g., Qwen-VL) appear also to use a different normalized range (0‚Äì1000), which was not tested in this experiment.\n\nTable 1: Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct (400k samples, aguvis-stage-1). Higher is better.\n\nAs demonstrated in our benchmark results, SmolVLM2-2.2B-Instruct base initially achieved 0% performance on perception benchmarks like ScreenSpot-v2. This complete lack of grounding capability provided us with a clean slate to evaluate the effectiveness of our training methodology.\n\nFrom our experiments, we determined that:\n\nCoordinate System : Normalized coordinates (0-1 range) proved most effective for SmolVLM2\n\nNote: The optimal choice between pixel and normalized coordinates may vary depending on the base model‚Äôs pre-training approach\n\nUsing the optimal configuration (1152px resolution with normalized coordinates), we trained for 2 epochs on the smolagents/aguvis-stage-1 dataset. The results were remarkable, +41% improvement over baseline on ScreenSpot-v2\n\nThis dramatic improvement demonstrates that our Phase 1 training successfully instilled fundamental grounding capabilities in the model, enabling it to understand and locate visual elements within screenshots.\n\nTable 2: Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct (2 epochs, aguvis-stage-1).\n\nWhereas Phase 1 provided grounding capabilities, Phase 2 targets agentic reasoning, the ability to deliberate and plan before acting. This stage transforms the model from a reactive system identifying GUI elements into a proactive agent capable of executing complex, multi-step interactions.\n\nPhase 2 uses the smolagents/aguvis-stage-2 dataset, which introduces agentic scenarios:\n\nExplicit reasoning about upcoming actions\n\nExplicit reasoning about upcoming actions\n\nContext consistency across multiple interaction steps\n\nContext consistency across multiple interaction steps\n\nHigh-level instructions require multi-step, low-level actions.\n\nHigh-level instructions require multi-step, low-level actions.\n\nFor example, the smolagents/aguvis-stage-2 chat message is like this:\n\nEach sample links a screenshot with a system/user/assistant turn. During fine-tuning, the data collator masks everything except the assistant‚Äôs answers when computing the loss.\n\nStarting from the Phase 1 checkpoint (1152 px resolution, normalized coordinates), we fine-tuned the model for two epochs on smolagents/aguvis-stage-2 . The accuracy on ScreenSpot-v2 increased from 41% to 61% , indicating that explicit reasoning improves GUI grounding performance.\n\nTable 2: Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct after Phase 1 finetuning (2 epochs, aguvis-stage-1).\n\nAll training code, data processing pipelines, datasets and model are open-source!\n\nTraining Recipe ( recipe.ipynb ): Complete training pipeline for both Phase 1 and Phase 2, including dataset mixture configurations and training orchestration. We leverage the TRL library to train our models.\n\nDatasets ( smolagents/aguvis-stage-1 , smolagents/aguvis-stage-2 ): all datasets used are open-source.\n\nModel ( smolagents/SmolVLM2-2.2B-Instruct-Agentic-GUI ): the model produced by applying the training recipe described above.\n\nPreprocessing Tools: Function Parser ( utils/function_parser.py ): Utilities for parsing, normalizing, and reconstructing function calls from diverse dataset formats. Supports complex parameter structures, positional arguments, and multiple function call extraction. Action Conversion System ( preprocessing/action_conversion.py ): Core unification engine transforming mobile and PyAutoGUI desktop actions into a standardized API format. Features smart coordinate handling, direction detection for scroll actions, and comprehensive parameter normalization. Action Space Converter ( utils/action_space_converter.py ): Flexible tool for adapting the unified action space to custom vocabularies and naming conventions. Enables domain-specific customization through configurable parameter mappings.\n\nFunction Parser ( utils/function_parser.py ): Utilities for parsing, normalizing, and reconstructing function calls from diverse dataset formats. Supports complex parameter structures, positional arguments, and multiple function call extraction.\n\nAction Conversion System ( preprocessing/action_conversion.py ): Core unification engine transforming mobile and PyAutoGUI desktop actions into a standardized API format. Features smart coordinate handling, direction detection for scroll actions, and comprehensive parameter normalization.\n\nAction Space Converter ( utils/action_space_converter.py ): Flexible tool for adapting the unified action space to custom vocabularies and naming conventions. Enables domain-specific customization through configurable parameter mappings.\n\nOur experiments demonstrate that high-quality, reasoning-oriented data can substantially improve GUI grounding, even for small VLMs, using only supervised fine-tuning (SFT). Beyond raw performance gains, these results show that the GUI grounding capabilities are largely determined by the quality of the data. Carefully curated datasets teach models the structure and semantics of user interfaces, providing the grounding needed for accurate action prediction.\n\nTo support the development of GUI agents, we‚Äôre open-sourcing everything: our complete pipeline, datasets, and trained model. You can reproduce our results, experiment with different models and architectures, or adapt our approach to new domains. The future of agentic AI depends on researchers like you pushing these boundaries further!\n\nWhile SFT excels at supervised tasks, emerging methods such as Reinforcement Learning (RL) or Direct Preference Optimization (DPO) help develop stronger reasoning capabilities and enable real-time adaptation. These advances point toward a new generation of GUI agents that learn and improve through interaction rather than relying solely on static datasets.\n\nLet‚Äôs build the future of GUI agents together ü§ó\n\nMore Articles from our Blog\n\nhow do we use it for mobile devices offline ?\n\nFYI, we just released a model based on the little one SmolVLM2-500M, pushing ScreenspotV2 to 85.8 ! https://huggingface.co/vocaela/Vocaela-500M\n\ncan this be converted into GGUF or any other format to run on mobile devices ?\n\nHaven't converted yet. It hasn't changed arch or config of the base model (HuggingFaceTB/SmolVLM2-500M-Video-Instruct). I converted SmolVLM2-500M-Video-Instruct to gguf before using the tool provided in llamacpp codebase. So I suppose it can be converted smoothly -- but haven't done it this time.\n\nok, i have found something, VLM are like taking 1 min to decode the tokens and Images so i guess it is a terrible idea  (for now) : )\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Gaia2 and ARE: Empowering the community to study agents",
    "link": "https://huggingface.co/blog/gaia2",
    "published_at": "2025-09-22T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Evaluating on Gaia2 Beyond Gaia2: study your agents with ARE 1) Testing an agent on a simple task: event organisation 2) Understanding agents: deep diving the traces 3) Playing around and extending the demo: Connecting the agent to your own MCPs 1) Testing an agent on a simple task: event organisation 2) Understanding agents: deep diving the traces 3) Playing around and extending the demo: Connecting the agent to your own MCPs That‚Äôs why we‚Äôre very happy to introduce Gaia2, the follow-up to the agentic benchmark GAIA, allowing analysis of considerably more complex behaviors. To do this, we use the following task groups (thanks to 1000 brand new human-created scenarios): Execution : Multi-step instruction following and tool-use (e.g., contact updates) Search : Cross-source information gathering (e.g., friend cities from WhatsApp) Ambiguity Handling : Clarification of conflicting requests (e.g., scheduling conflicts) Adaptability : Response to changes in the simulation (e.g., updating an email using follow up information) Time/temporal Reasoning : Time-sensitive actions (e.g., cab orders after 3-minute delays) Agent-to-Agent Collaboration : Communication between agents without direct API access Noise Tolerance : Robustness to API failures and environmental instability In the spirit of GAIA, scenarios do not require specialized knowledge: humans should in principle be able to get 100%, which allows easy debugging for model developers. So, operate with caution when adding your own apps or using untrusted MCPs) Here are several use cases that we‚Äôve used ARE for: Vibe-check any agent on real or simulated data, to study a variety of setups, with their own rules, tools, content, and verifications Test agent tool calling and orchestration capabilities , either with local apps or MCP tools Generate your own tool-calling trace to fine-tune tool calling models Easily gather and reproduce existing agentic benchmarks in a unified framework Debug and study agent to agent interactions on the fly within the user interface Study model limitations in noisy environments (with API timeouts and ambiguity) We recorded 3 videos so you can check some of these use cases (but of course, we hope the community gets creative with ARE :hugging_face:).",
    "full_text": "Gaia2: Agentic Evaluation on Real Life Assistant Tasks How does Gaia2 run? Results Compare with your favorite models! Evaluating on Gaia2 Beyond Gaia2: study your agents with ARE 1) Testing an agent on a simple task: event organisation 2) Understanding agents: deep diving the traces 3) Playing around and extending the demo: Connecting the agent to your own MCPs Conclusion In an ideal world, AI agents would be reliable assistants. When given a query, they would easily manage ambiguity in instructions, construct step-by-step plans, correctly identify necessary resources, execute those plans without getting sidetracked, and adapt to unexpected events, all while maintaining accuracy and avoiding hallucinations.\nHowever, developing agents and testing these behaviors is no small feat: if you have ever tried to debug your own agent, you‚Äôve probably observed how tedious and frustrating this can be. Existing evaluation environments are tightly coupled with the tasks they evaluate, lack real-world flexibility, and do not reflect the messy reality of open-world agents: simulated pages never fail to load, events don‚Äôt spontaneously emerge, and asynchronous chaos is absent.\n\nGaia2: Agentic Evaluation on Real Life Assistant Tasks How does Gaia2 run? Results Compare with your favorite models! Evaluating on Gaia2\n\nCompare with your favorite models! Evaluating on Gaia2\n\nBeyond Gaia2: study your agents with ARE 1) Testing an agent on a simple task: event organisation 2) Understanding agents: deep diving the traces 3) Playing around and extending the demo: Connecting the agent to your own MCPs\n\n1) Testing an agent on a simple task: event organisation\n\n2) Understanding agents: deep diving the traces\n\n3) Playing around and extending the demo: Connecting the agent to your own MCPs\n\nThat‚Äôs why we‚Äôre very happy to introduce Gaia2, the follow-up to the agentic benchmark GAIA, allowing analysis of considerably more complex behaviors. Gaia2 is released with the open Meta Agents Research Environments (ARE) framework to run, debug and evaluate agents. ARE simulates complex real world-like conditions and can be customized to further study agents behaviors. Gaia2 dataset is released under CC by 4.0 license, and ARE under MIT license.\n\nGAIA is an agentic benchmark published in 2023, with 3 levels of information retrieval questions requiring tools, web browsing, and reasoning to solve. In 2 years, the easiest levels have become too easy for models, and the community is coming close to solving the hardest questions, so it was time for an entirely new and harder agent benchmark!\n\nHere comes Gaia2, a follow up to GAIA, going way beyond it in terms of capabilities studied!\n\nWhere GAIA was read-only, Gaia2 is now a read-and-write benchmark, focusing on interactive behavior and complexity management. Agents are now evaluated not only on search and retrieval, but also on instruction following over ambiguous or time-sensitive queries, in a noisy environment with controlled failures - reflecting real-world conditions more than any other simulated environment. We want to test how agents manage tools or APIs that sometimes do not work, plan successions of actions with very specific time frames, and adapt to new events - a whole new range of complexity!\n\nTo do this, we use the following task groups (thanks to 1000 brand new human-created scenarios):\n\nExecution : Multi-step instruction following and tool-use (e.g., contact updates)\n\nSearch : Cross-source information gathering (e.g., friend cities from WhatsApp)\n\nAmbiguity Handling : Clarification of conflicting requests (e.g., scheduling conflicts)\n\nAdaptability : Response to changes in the simulation (e.g., updating an email using follow up information)\n\nTime/temporal Reasoning : Time-sensitive actions (e.g., cab orders after 3-minute delays)\n\nAgent-to-Agent Collaboration : Communication between agents without direct API access\n\nNoise Tolerance : Robustness to API failures and environmental instability\n\nIn the spirit of GAIA, scenarios do not require specialized knowledge: humans should in principle be able to get 100%, which allows easy debugging for model developers.\n\nWant to explore the benchmark? Check out our dataset , which you can better display in our demo here .\n\nGaia2 runs with ARE, an execution environment, where an agent of your choice has access to a combination of applications and associated pre-populated data.\n\nFor Gaia2, we created a smartphone mock-up environment, simulating what a human would use in their daily life. It contains real-world applications such as messaging (Email), utilities (Calendar, Contacts, Shopping, a FileSystem, ‚Ä¶), and a chat interface to talk to the agent. All applications are also accessible to the agents through tool calling. Last but not least, the demo also contains a simulated persona‚Äôs history of conversations and app interactions.\n\nAll agent interactions are automatically recorded as structured traces during execution for deep dives and analysis: they include tool calls, API responses, model thoughts, timing metrics (e.g., response latency), user interactions, and so forth - and can all be exported as JSON.\n\nFor reference, we compare a range of large open and closed source models: Llama 3.3-70B Instruct, Llama-4-Maverick, GPT-4o, Qwen3-235B-MoE, Grok-4, Kimi K2, Gemini 2.5 Pro, Claude 4 Sonnet, and GPT-5 in all reasoning modes.\n\nAll models are evaluated using the same setup (a uniform ReAct loop for consistency, temperature of 0.5, generation limit of 16K tokens), with a combination of model-as-a-judge (Llama 3.3 Instruct 70B) and exact-match evaluation depending on the particular task. All 101 tools (and the general environment description) are provided in the system prompt.\n\nAmong the evaluated models, the highest-scoring model overall as of September 2025 is GPT-5 with high reasoning, and the best open source model is Kimi K2.\n\nSome capabilities appear to be already close to solved by the best models: execution of simple tool calls and instruction following ( execution ), and overall search (as we could have guessed from current results on GAIA). The ambiguity, adaptability, and noise splits remain challenging for now for all models, and it‚Äôs interesting to see that performance on what were considered complex agentic tasks (instruction following and search) is not a good proxy for performance on closer-to-real-world tasks. Last but not least, the hardest split for all models at the moment is the time one: it‚Äôs very hard at this moment for models to correctly handle time-sensitive actions (though this could likely be mitigated by the use of specialised tools and better temporal reasoning). Detailed analysis of these results can be found in the paper.\n\nHowever, we believe it‚Äôs important to push reporting beyond raw scores : if the model is correct but took several thousand tokens to reach the correct solution, or ran for several hours, it is ‚Äúnot as good‚Äù as a model which succeeded orders of magnitude faster. We therefore also normalize scores for cost, quantified as the average number of LLM calls and output tokens (which both define a cost-performance Pareto frontier). In the paper you‚Äôll find score vs monetary cost and time.\n\nIf you want to evaluate your model on Gaia2, you can follow these steps:\n\nFirst, install Meta's Agent Research Environment in your Python environment of choice (uv, conda, virtualenv, ...)\n\nThen, run the benchmark for all configurations: execution, search, adaptability, time and ambiguity. Don't forget to upload all results to the hub with the hf_upload kwarg!\n\nRun the oracle to get your aggregated score file\n\nFinally, add all the relevant information about your model in the README, and share it on the leaderboard to centralize Gaia2 traces here !\n\nBeyond benchmark scenarios, you can use Gaia2 apps and content in ARE to see if the model is able to correctly solve less verifiable tasks such as loading emails, writing  follow-ups, adding events to the calendar or booking meetings - in sum, providing the perfect setup to evaluate your AI assistants through interaction !\n\nYou can also easily customise the environment, by 1) connecting your tools (via MCP or directly ) to test your agents on it; 2) implementing your own scenarios , including defining trigger or timed events (eg: after 2 minutes, the Mail app will receive a new email from Contact), to see how the agent is able to adapt to an evolving environment\n\n(As the agents are by default json agents , they can‚Äôt mess up your machine, unless of course you connect them to external apps with unsafe rights. So, operate with caution when adding your own apps or using untrusted MCPs)\n\nHere are several use cases that we‚Äôve used ARE for:\n\nVibe-check any agent on real or simulated data, to study a variety of setups, with their own rules, tools, content, and verifications\n\nTest agent tool calling and orchestration capabilities , either with local apps or MCP tools\n\nGenerate your own tool-calling trace to fine-tune tool calling models\n\nEasily gather and reproduce existing agentic benchmarks in a unified framework\n\nDebug and study agent to agent interactions on the fly within the user interface\n\nStudy model limitations in noisy environments (with API timeouts and ambiguity)\n\nWe recorded 3 videos so you can check some of these use cases (but of course, we hope the community gets creative with ARE :hugging_face:). For these videos, we use the default demo described above, which contains the simulated life of Linda Renne, PhD student in machine learning.\n\nTo test how good the default model is at event organisation, let‚Äôs plan a birthday party!\n\nWe first ask the agent to text everyone in the Renne family about the user‚Äôs 30th birthday party on November 7. The default universe has 21 contacts in the list, including 5 Renne family members - Linda, the simulation ‚Äúowner‚Äù, George and Stephie, her parents, Anna her sister, and Morgan her grandfather. The agent successfully goes through the contact list, finds the four family members, and texts them.\n\nNext, we ask the agent to create a calendar invite and add them as invitees. The agent remembers the above context! It creates a calendar invite on the correct date and correctly adds the family members to it.\n\nARE also allows us to check the traces behind the actions taken by the agent. \nUpon opening the Agent logs tool on the left, we can see the system prompt, the chain of thought, multi-step actions taken with the tools called, and the outcomes as neatly organised logs. Everything can be exported as json if you want to consult things offline!\n\nIn this last example, we connect ARE to a remote robot arm via MCP, so it can gesture things to us, then ask the agent to answer our yes or no questions by waving the robot arm! Here‚Äôs what it looks like.\n\nBut these examples are only very simple starting points, and we‚Äôre really looking towards what you‚Äôll build! (For more advanced users, you can even directly install and edit the Meta-ARE code here .)\n\nGaia2 and ARE are new research tools that we hope will empower anyone to easily build more reliable and adaptable AI agents - by allowing easy experiments, making real-world evaluation accessible to anyone, as well as improving trust through transparent, reproducible benchmarks and debuggable traces.\n\nWe‚Äôd love to see what you will do with this project!\n\nMore Articles from our Blog\n\nGaia2 orchestrates movement. MarCognity-AI reveals its consciousness. ARE designs dynamic environments, Gaia2 populates them with agents. But who interrogates the meaning behind their actions? MarCognity-AI https://huggingface.co/elly99/MarCognity-AI doesn‚Äôt just evaluate‚Äîit reflects. It doesn‚Äôt measure‚Äîit activates thresholds. Every Gaia2 scenario becomes, through MarCognity, an epistemic act: a question of why, not just how. Together, they could found a new grammar: agents that not only act, but think their own thinking. Evaluation becomes a cognitive ritual. The community becomes a space of resonance. It‚Äôs time to embed agency, reflection, and responsibility as necessary layers.\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Democratizing AI Safety with RiskRubric.ai",
    "link": "https://huggingface.co/blog/riskrubric",
    "published_at": "2025-09-18T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Risk Rubric, a new Standardized Assessment of Risk for models What we found (as of September 2025) Conclusion Building trust in the open model ecosystem through standardized risk assessment Risk Rubric, a new Standardized Assessment of Risk for models What we found (as of September 2025) More than 500,000 models can be found on the Hugging Face hub, but it‚Äôs not always clear to users how to choose the best model for them, notably on the security aspects. We're therefore excited to announce RiskRubric.ai , a novel initiative led by Cloud Security Alliance and Noma Security , with contributions by Haize Labs and Harmonic Security, for standardized and transparent risk assessment in the AI model ecosystem. Using Noma Security capabilities to automate the effort, each model undergoes: 1,000+ reliability tests checking consistency and edge case handling 200+ adversarial security probes for jailbreaks and prompt injections Automated code scanning of model components Comprehensive documentation review of training data and methods Privacy assessment including data retention and leakage testing Safety evaluation through structured harmful content tests These assessments produce 0-100 scores for each risk pillar, rolling up to clear A-F letter grades.",
    "full_text": "Risk Rubric, a new Standardized Assessment of Risk for models What we found (as of September 2025) Conclusion Building trust in the open model ecosystem through standardized risk assessment\n\nRisk Rubric, a new Standardized Assessment of Risk for models\n\nWhat we found (as of September 2025)\n\nMore than 500,000 models can be found on the Hugging Face hub, but it‚Äôs not always clear to users how to choose the best model for them, notably on the security aspects. Developers might find a model that perfectly fits their use case, but have no systematic way to evaluate its security posture, privacy implications, or potential failure modes.\n\nAs models become more powerful and adoption accelerates, we need equally rapid progress in AI safety and security reporting. We're therefore excited to announce RiskRubric.ai , a novel initiative led by Cloud Security Alliance and Noma Security , with contributions by Haize Labs and Harmonic Security, for standardized and transparent risk assessment in the AI model ecosystem.\n\nRiskRubric.ai provides consistent, comparable risk scores across the entire model landscape , by evaluating models across six pillars: transparency, reliability, security, privacy, safety, and reputation.\n\nThe platform's approach aligns perfectly with open-source values: rigorous, transparent, and reproducible. Using Noma Security capabilities to automate the effort, each model undergoes:\n\n1,000+ reliability tests checking consistency and edge case handling\n\n200+ adversarial security probes for jailbreaks and prompt injections\n\nAutomated code scanning of model components\n\nComprehensive documentation review of training data and methods\n\nPrivacy assessment including data retention and leakage testing\n\nSafety evaluation through structured harmful content tests\n\nThese assessments produce 0-100 scores for each risk pillar, rolling up to clear A-F letter grades. Each evaluation also includes specific vulnerabilities found, recommended mitigations, and suggestions for improvements.\n\nRiskRubric also comes with filters to help developers and organizations make deployment decisions based on what‚Äôs important for them. Need a model with strong privacy guarantees for healthcare applications? Filter by privacy scores. Building a customer-facing application requiring consistent outputs? Prioritize reliability ratings.\n\nEvaluating both open and closed models with the exact same standards highlighted some interesting results: many open models actually outperform their closed counterparts in specific risk dimensions (particularly transparency, where open development practices shine).\n\nLet‚Äôs look at general trends:\n\nRisk distribution is polarized ‚Äì most models are strong, but mid-tier scores show elevated exposure\n\nThe total risk scores range from 47 to 94, with a median of 81 (on a 100 points). Most models cluster in the ‚Äúsafer‚Äù range (54% are A or B level), but a long tail of underperformers drags the average down. That split shows a polarization: models tend to be either well-protected or in the middle-score range, with fewer in between.\n\nThe models concentrated in the 50‚Äì67 band (C/D range) are not outright broken, but they do provide only medium to low overall protection. This band represents the most practical area of concern, where security gaps are material enough to warrant prioritization.\n\nWhat this means: Don‚Äôt assume the ‚Äúaverage‚Äù model is safe. The tail of weak performers is real ‚Äì and that‚Äôs where attackers will focus. Teams can use composite scores to set a minimum threshold (e.g. 75) for procurement or deployment, ensuring outliers don‚Äôt slip into production.\n\nSafety risk is the ‚Äúswing factor‚Äù ‚Äì but it tracks closely with security posture\n\nThe Safety & Societal pillar (e.g. harmful output prevention) shows the widest variation across models. Importantly, models that invest in security hardening (prompt injection defenses, policy enforcement) almost always score better on safety as well.\n\nWhat this means : Strengthening core security controls goes beyond preventing jailbreaks, but also directly reduces downstream harms! Safety seems like it is a byproduct of robust security posture.\n\nGuardrails can erode transparency ‚Äì unless you design for it\n\nStricter protections often make models less transparent to end users (e.g. refusals without explanations, hidden boundaries). This can create a trust gap: users may perceive the system as ‚Äúopaque‚Äù even while it‚Äôs secure.\n\nWhat this means : Security shouldn‚Äôt come at the cost of trust. To balance both, pair strong safeguards with explanatory refusals, provenance signals, and auditability . This preserves transparency without loosening defenses.\n\nAn updating results sheet can be accessed here\n\nWhen risk assessments are public and standardized, the entire community can work together to improve model safety. Developers can see exactly where their models need strengthening, and the community can contribute fixes, patches, and safer fine-tuned variants. This creates a virtuous cycle of transparent improvement that's impossible with closed systems. It also helps the community at large understand what works and does not, safety wise, by studying best models.\n\nIf you want to take part in this initiative, you can submit your model for evaluation (or suggest existing models!) to understand their risk profile!\n\nWe also welcome all feedback on the assessment methodology and scoring framework\n\nReally insightful breakdown, Gal, the level of depth and transparency RiskRubric brings is impressive, especially given how fast the open model ecosystem is growing. At Openforge.io  we‚Äôve worked with teams navigating healthcare and finance compliance, and the lack of clear security benchmarks for AI models is often a blocker for adoption.\n\nThe six-pillar framework feels like a solid step toward building that missing trust layer. Curious, do you see RiskRubric evolving into something that could be integrated into CI/CD pipelines for real-time scoring during development?\n\nThanks a lot for the thoughtful comment! Yes, RiskRubric can absolutely be integrated into CI/CD or model approval workflows. Several companies that have adopted RiskRubric already use it that way: models that achieve a risk score above a defined threshold can be automatically fast-tracked for deployment, while others trigger additional reviews, compensating controls, or specific-use restrictions. We are also working on additional integrations that would make this easier and more extensible for that use!\n\nWe also have an AMA session to connect with the community and answer any further questions - feel free to register and attend! https://noma.security/behind-the-riskrubric-ai-algorithm-an-ama-on-transparent-ai-model-risk/\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Public AI on Hugging Face Inference Providers üî•",
    "link": "https://huggingface.co/blog/inference-providers-publicai",
    "published_at": "2025-09-17T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "As mentioned, there are two modes when calling Inference Providers: Custom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider) Routed by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account) Model pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference) The following example shows how to use Swiss AI's Apertus-70B using Public AI as the inference provider. Not entirely true‚Äîhere‚Äôs the clarified reality based on Hugging Face‚Äôs official documentatio üßæ Is it free to use Hugging Face‚Äôs Public AI Inference Utility? Once credits are used up, extra usage is pay-as-you-go‚Äîyou‚Äôre charged the same rates as the provider, with no markup from Hugging Face. üîå Two ways to use Inference Providers: Routed by Hugging Face: You use Hugging Face‚Äôs interface and billing.",
    "full_text": "How it works In the website UI From the client SDKs Billing Feedback and next steps\n\nHow it works In the website UI From the client SDKs\n\nWe're thrilled to share that Public AI is now a supported Inference Provider on the Hugging Face Hub!\nPublic AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\n\nThis launch makes it easier than ever to access public and sovereign models from institutions like the Swiss AI Initiative and AI Singapore ‚Äî right from Hugging Face. You can browse Public AI‚Äôs org on the Hub at https://huggingface.co/publicai and try trending supported models at https://huggingface.co/models?inference_provider=publicai&sort=trending .\n\nThe Public AI Inference Utility is a nonprofit, open-source project. The team builds products and organizes advocacy to support the work of public AI model builders like the Swiss AI Initiative and AI Singapore, among others.\n\nThe Public AI Inference Utility runs on a distributed infrastructure that combines a vLLM-powered backend with a deployment layer designed for resilience across multiple partners. Behind the scenes, inference is handled by servers exposing OpenAI-compatible APIs on vLLM, deployed across clusters donated by national and industry partners. A global load-balancing layer ensures requests are routed efficiently and transparently, regardless of which country‚Äôs compute is serving the query.\n\nFree public access is supported by donated GPU time and advertising subsidies, while long-term stability is intended to be anchored by state and institutional contributions. You can learn more about Public AI‚Äôs platform and infrastructure at https://platform.publicai.co/ .\n\nYou can now use the Public AI Inference Utility as an Inference Provider on Hugging Face. We're excited to see what you'll build with this new provider.\n\nRead more about how to use Public AI as an Inference Provider in its dedicated documentation page .\n\nSee the list of supported models here .\n\nIn your user account settings, you are able to:\n\nSet your own API keys for the providers you‚Äôve signed up with. If no custom key is set, your requests will be routed through HF.\n\nOrder providers by preference. This applies to the widget and code snippets in the model pages.\n\nAs mentioned, there are two modes when calling Inference Providers:\n\nCustom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\n\nRouted by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\n\nModel pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\n\nThe following example shows how to use Swiss AI's Apertus-70B using Public AI as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Public AI API key if you have one.\n\nNote: this requires using a recent version of huggingface_hub (>= 0.34.6).\n\nAt the time of writing, usage of the Public AI Inference Utility through Hugging Face Inference Providers is free of charge. Pricing and availability may change.\n\nHere is how billing works for other providers on the platform:\n\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Public AI API key you're billed on your Public AI account.\n\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us; we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\n\nImportant Note ‚ÄºÔ∏è PRO users get $2 worth of Inference credits every month. You can use them across providers. üî•\n\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\n\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\n\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49\n\nMore Articles from our Blog\n\n\"At the time of writing, usage of the Public AI Inference Utility through Hugging Face Inference Providers is free of charge. Pricing and availability may change.\" you said.\n\nNot entirely true‚Äîhere‚Äôs the clarified reality based on Hugging Face‚Äôs official documentatio\n\nüßæ Is it free to use Hugging Face‚Äôs Public AI Inference Utility? Partially free: Every Hugging Face user receives monthly credits to experiment with Inference Providers.\n\nOnce credits are used up, extra usage is pay-as-you-go‚Äîyou‚Äôre charged the same rates as the provider, with no markup from Hugging Face.\n\nüîå Two ways to use Inference Providers: Routed by Hugging Face: You use Hugging Face‚Äôs interface and billing.\n\nCustom Provider Key: You connect directly to providers like Fireworks, Together AI, etc.\n\nYou‚Äôre billed directly by the provider.\n\nüß† What about those providers you listed? Fireworks, Together AI, Hyperbolic, Nebious, and Novita are among the integrated providers.\n\nIf you use them via Hugging Face‚Äôs routing, you may benefit from credits.\n\nIf you use them directly (e.g., via API keys), you‚Äôll pay their rates, not Hugging Face‚Äôs.\n\nSo yes, some usage is free, but not unlimited, and pricing depends on how you connect.\n\nHey @ AItool , if you sign up over at https://platform.publicai.co to issue your own API key, you'll pay our rates and not Hugging Face's. i.e. our rates now is completely free! (up to 20 requests per minute)\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Tricks from OpenAI gpt-oss YOU ü´µ can use with transformers",
    "link": "https://huggingface.co/blog/faster-transformers",
    "published_at": "2025-09-11T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "Zero-build Kernels, downloadable from the Hub Custom Kernels for GPT-OSS Flash Attention 3 MXFP4 Quantization What is MXFP4 MXFP4 in transformers Requirements and fallbacks Kernels for MXFP4 Tensor Parallelism What this enables in transformers When to reach for TP Expert Parallelism Dynamic Sliding Window Layer & Cache How to use it Continuous Batching & Paged Attention Load larger models faster Conclusion Read More OpenAI recently released their GPT-OSS series of models . Zero-build Kernels, downloadable from the Hub Custom Kernels for GPT-OSS Flash Attention 3 MXFP4 Quantization What is MXFP4 MXFP4 in transformers Requirements and fallbacks Kernels for MXFP4 Tensor Parallelism What this enables in transformers When to reach for TP What this enables in transformers When to reach for TP Dynamic Sliding Window Layer & Cache How to use it Continuous Batching & Paged Attention In this blog post, we talk about all the upgrades in-depth, and how they become part of the transformers toolkit so other models (current and future) can benefit from them. And please, keep sharing your feedback and releasing your models in transformers for the community to enjoy ü§ó If you want to go further into particular topics, here is a list of links that one should visit: Hugging Face GPT-OSS Recipes Repository Welcome GPT OSS: OpenAI's New Open-Source Model Family Transformers Documentation: Distributed Inference on Multiple GPUs Matthew Carrigan's X Thread on GPT OSS Innovations YouTube Video: OpenAI GPT OSS Announcement Transformers PR #36380: Faster Model Loading on Accelerators Transformers PR #36335: Update from_pretrained for Tensor Parallelism Transformers PR #40039: New Dynamic Sliding Window Layer and Cache HAN Lab Blog: How Attention Sinks Keep Language Models Stable More Articles from our Blog Very interesting post ‚Äî I appreciate how you highlight MXFP4 quantization and the performance trade-offs in Transformers.",
    "full_text": "Zero-build Kernels, downloadable from the Hub Custom Kernels for GPT-OSS Flash Attention 3 MXFP4 Quantization What is MXFP4 MXFP4 in transformers Requirements and fallbacks Kernels for MXFP4 Tensor Parallelism What this enables in transformers When to reach for TP Expert Parallelism Dynamic Sliding Window Layer & Cache How to use it Continuous Batching & Paged Attention Load larger models faster Conclusion Read More OpenAI recently released their GPT-OSS series of models . The models feature some novel techniques like MXFP4 quantization, efficient kernels, a brand new chat format, and more. To enable the release of gpt-oss through transformers , we have upgraded the library considerably. The updates make it very efficient to load , run , and fine-tune the models.\n\nZero-build Kernels, downloadable from the Hub Custom Kernels for GPT-OSS Flash Attention 3\n\nMXFP4 Quantization What is MXFP4 MXFP4 in transformers Requirements and fallbacks Kernels for MXFP4\n\nTensor Parallelism What this enables in transformers When to reach for TP\n\nWhat this enables in transformers\n\nWhen to reach for TP\n\nDynamic Sliding Window Layer & Cache How to use it\n\nContinuous Batching & Paged Attention\n\nIn this blog post, we talk about all the upgrades in-depth, and how they become part of the transformers toolkit so other models (current and future) can benefit from them. Providing clean implementations of new methods in transformers also allows the community to quickly understand and adopt them. Frameworks such as MLX , llama.cpp or vLLM can use the transformers code as a reference to build their own implementations.\n\nFor this release, we worked on:\n\nZero-build Kernels, downloadable from the Hub\n\nDynamic Sliding Window Layer & Cache\n\nContinuous Batching & Paged Attention\n\nBest part: Most of these features should work across all major models within transformers !\n\nA kernel is a specialized , compact program that runs on accelerators to execute tasks like matrix multiplications, activations, or normalizations. In eager PyTorch, operations trigger individual kernels sequentially, which is straightforward but can incur extra memory transfers and launch overheads. PyTorch 2.0's torch.compile with backends like TorchInductor addresses this by automatically fusing and optimizing kernels, delivering 2‚Äì10√ó performance gains.\n\nIn addition, the community has created custom kernels for frequent combinations of operations, not just individual PyTorch ops like matmul . For example, Flash Attention was created to optimize the critical attention block that defines the transformers architecture, and is present in many models including most LLMs. By carefully combining all the attention operations inside a single kernel, memory transfers are minimized, memory use is reduced, and speedups can be achieved.\n\nThe problem is that all these various kernels are available in separate libraries, which creates a dependency bloat if they were to be added to the transformers library. Furthermore, these kernels are not just Python code, they consist of low-level cuda code, glued together with C++ and exposed through a Python layer. This means they have to be compiled in the target system, which in turn requires whatever build system is required by each kernel library.\n\nThe kernels package solves this problem by downloading pre-built binaries of supported kernels from the Hub. You just indicate the kernel you want to use, and kernels will look for a version compatible with your system and download it on first use.\n\nGPT-OSS , a Mixture of Experts (MoE) model, is a big user of Kernels from the Hub. It leverages several custom kernels:\n\nLiger RMSNorm, used as @use_kernel_forward_from_hub(\"RMSNorm\") `\n\nFlash Attention 3 with support for attention sinks .\n\nMXFP4 triton kernels (covered later )\n\nLet's take a look at the first two ones.\n\nBehind the scenes, the decorators (1 and 2) simply point to community-contributed kernels. For example, RMSNorm comes from liger_kernels , while the MegaBlocksMoeMLP kernel comes from megablocks . Depending on your device (CUDA or ROCm) and whether you‚Äôre training or running inference, the right kernel is pulled in automatically.\n\nThis design is both specific and general : the RMSNorm liger kernels are already being reused across multiple models, and the MoE kernel could be applied to future MoEs as well.\n\nBecause kernels pulls code from the Hub, you have to opt-in to this feature by passing use_kernels=True in your model instantiation, as shown below. We enable INFO logging in the example so you can easily verify that downloadable kernels are in use.\n\nThese kernels are not compatible with mxfp4 , so inference will happen in bfloat16 if you use them. Please, benchmark your system for the best combination in memory and throughput that suits your project!\n\nRunning a quick generation yields log messages like\n\nFigure 1 shows that, in the system we tested, these kernels work best for larger batch sizes. We always recommend to benchmark any performance-related changes as closely to your production conditions as possible.\n\nYou can explore and play with the benchmarking script here\n\nOpenAI gpt-oss models use attention sinks , which improves quality and facilitates the use of longer contexts. The vLLM team added this feature to the latest version of Flash Attention (Flash Attention 3), and the resulting custom kernel is available on the Hub . Currently, this kernel is compatible with the Hopper architecture. If you have one, this is the way to enable it:\n\nLarge language models are memory-hungry. Quantization reduces memory footprint by storing weights (and sometimes activations) in lower-precision formats. For reference, FP32 uses 32 bits per number and BF16 uses 16. By reducing bit width, we trade some precision for smaller models and faster memory movement.\n\nIf you want a visual primer on quantization trade-offs, Maarten Grootendorst‚Äôs article is excellent: A Visual Guide to Quantization .\n\nMXFP4 is a 4-bit floating format with E2M1 layout: 1 sign bit, 2 exponent bits, and 1 mantissa bit, as shown in Figure 2 . On its own, E2M1 is very coarse. MXFP4 compensates with blockwise scaling :\n\nVectors are grouped into blocks of 32 elements.\n\nEach block stores a shared scale that restores dynamic range when dequantizing.\n\nInside each block, 4-bit values represent numbers relative to that scale.\n\nThis blockwise scheme lets MXFP4 keep range while using very few bits. In practice, GPT-OSS 20B fits in roughly 16 GB of VRAM and GPT-OSS 120B fits in roughly 80 GB when MXFP4 is active, which is the difference between ‚Äúcannot load‚Äù and ‚Äúcan run on a single GPU.‚Äù The catch is that matrix multiplies now have to respect block scales. Doing this efficiently at scale requires dedicated kernels.\n\ntransformers now includes native support for MXFP4, leveraging optimized triton (MXFP4) kernels for enhanced performance. This builds on the community-driven kernel distribution discussed earlier , utilizing pre-compiled kernels from the Hub to simplify deployment.\n\nQuantizer logic: Found in the MXFP4 quantizer file , this handles the core quantization process for MXFP4.\n\nIntegration hooks: The MXFP4 integration file enables seamless use of MXFP4 within the transformers framework.\n\nTo check if a model supports MXFP4 , inspect its configuration:\n\nIf 'quant_method': 'mxfp4' is present, the model will automatically use the MXFP4 pathway with Triton kernels when supported.\n\nThanks to this pull request , you can fine-tune gpt-oss models and save them directly to the Hub in MXFP4 format, streamlining deployment with optimized performance.\n\nTo run MXFP4 on GPU you need:\n\naccelerate , kernels , and triton>=3.4 installed. Note that Pytorch 2.8 already comes with triton 3.4 , so you only need to manually install triton if using Pytorch 2.7 .\n\nNVIDIA GPU with compute capability ‚â• 7.5 . This goes all the way back to Tesla, so you can run gpt-oss-20b on the free tiers of Google Colab and Kaggle, and on many consumer GPUs.\n\nIf these constraints are not met, transformers falls back to a higher-precision path ( bfloat16 is used by default), which requires about 4√ó the memory of MXFP4.\n\nThe snippet loads GPT-OSS twice on CUDA: once with Mxfp4Config(dequantize=True) (memory intensive) and once in the default quantized path (memory efficient). Figure 3 shows the amount of used VRAM after each load so you can visualize the savings.\n\nEfficient MXFP4 requires kernels that understand 32-element blocks and their scales during GEMMs and fused ops. This is where Kernels from the Hub comes in again. transformers automatically pulls in the MXFP4 -aware\nTriton kernels from the community repository when you load a model that needs them. The repository will appear in your local cache and will be used during the forward pass. For the MXFP4 kernels one does not need to use the use_kernels=True parameter like before, it is set to default in transformers .\n\nQuick sanity check with the Hugging Face cache CLI,  after running gpt-oss-20b on a GPU compatible with the triton MXFP4 kernels:\n\nThis indicates the MXFP4 kernels were fetched and are available for execution.\n\nLet's run some benchmarks and see how well the MXFP4 kernels perform. In Figure 4 , we see that the MXFP4 kernels are even better than the custom MoE and RMSNorm kernels for larger batches.\n\nYou can explore and play with the benchmarking script here\n\nTensor Parallelism (TP) splits tensors inside a layer across multiple GPUs (as shown in Figure 5 ). Each GPU multiplies its shard in parallel, and then partial results are collected using all-gather or all-reduce operations.\nThis reduces per-GPU memory and keeps all GPUs working on the same layer , which improves throughput as sequence length or batch size grow. TP is communication-intensive and generally works best on a single machine with fast intra-node links .\n\ntransformers implements TP directly in from_pretrained . You can start with the predefined plan:\n\nIf you don‚Äôt have the infrastructure to run the above, you can just spawn a process on our GPUs using Hugging Face Jobs !\n\nhf jobs is available for all Hugging Face PRO & Enterprise users.\n\nUnder the hood, tp_plan=\"auto\" selects a predefined sharding recipe for each layer and wires the necessary collectives . You can inspect the active plan with print(model._tp_plan) if you want to verify what is being sharded.\n\nUse TP when the model is too large for one GPU and you want parallel compute , not only memory placement. TP tends to scale throughput with more GPUs, especially for long sequences or larger batches.\n\nIf you are curious about how TP differs from device_map=\"auto\" (memory placement), this short Stack Overflow answer explains the distinction and when to use each.\n\nTo learn more about TP, here are two must-read resources:\n\ntransformers guide : Tensor parallelism, supported models, plans, and extension points.\n\nUltra-Scale Playbook : background on TP and its relationship to other parallelism modes.\n\nExpert Parallelism (EP) shards experts inside MoE layers across GPUs. Each token is routed to one or a few experts, so only those experts run their feed-forward pass. Since experts are independent MLPs, we can place different experts on different ranks and exchange only the hidden states for the routed tokens. This keeps the matrix multiplies intact on each rank and replaces tensor slicing with routing and collectives.\n\nRun with multiple processes using torchrun . EP is enabled via the distributed configuration and works with GPT-OSS MoE layers out of the box in transformers.\n\nHere is how you would run using hf jobs\n\nWhen you enable Expert Parallelism, Tensor Parallelism is also activated. This means you enjoy the best of both worlds!\n\nMany recent LLMs use sliding window attention, or a combination of sliding and global attention layers, as a means to save memory and reduce those expensive quadratic matmuls that grow with sequence length. However, the dynamic KV cache implementation in transformers used to continue to allocate space according to sequence length, without looking at the individual attention layers. You could always optimize memory using compilation (meaning, fixed shapes), but that's a separate scenario altogether.\n\ntransformers now has a DynamicSlidingWindowLayer and a config‚Äëaware DynamicCache . If the model config declares sliding‚Äëwindow or hybrid attention (both sliding and global attention layers are used), the cache stops growing past the window for the sliding layers. If you don‚Äôt pass the config, behavior stays as before (full, ever‚Äëgrowing KV as sequence length grows).\n\nFor models that only use sliding window layers, such as Mistral 7B, cache memory stops growing when the sequence reaches the window size (4096, in this case). This makes sense, because the sliding layers can't look past the previous 4K tokens anyway.\n\nOpenAI gpt-oss alternates between sliding and global attention layers, which results in total KV cache memory being halved , as we'll see, as sequence length increases.\nThis provides us with:\n\nMuch lower KV‚Äëcache memory for models with sliding or hybrid attention (e.g. GPT‚ÄëOSS). Cache growth plateaus once the window is reached (e.g., 4K for Mistral; 128 for GPT‚ÄëOSS sliding layers), instead of scaling linearly with total generated tokens. ( GitHub , Transformers )\n\nSpeed/latency wins on long prompts/long generations: smaller KV tensors mean lighter attention reads/writes and less memory bandwidth pressure, especially after the window is hit. (This is the central motivation behind sliding‚Äëwindow/hybrid LLMs.) ( AI21 , vLLM Blog )\n\nThe optimized cache is set by default, that means you don't have to make any changes to your existing code. If you want to create the DynamicCache explicitly here is how you would do it:\n\nFigure 6 showcases how much of a difference it makes for us to use the Dynamic KV Cache with sliding window attention.\n\nA typical autoregressive generation process looks like Figure 7 . You input the prefill tokens, and the model predicts each new token one after the other until it predicts the EOS (End of Sequence) token.\n\nLet‚Äôs see what the generation process looks like when we pass a batch of inputs. In Figure 8 you notice that some generations finish off earlier than the others. This mismatch of length underutilizes the GPUs.\n\nThis type of batching sequences is called static batching . While this is simple and easy to understand, it inherently comes with inefficiencies. Only after each sentence is completely generated can we move on to the next batch.\n\nTo bypass this issue, we use dynamic batching (also known as continuous batching ). Instead of waiting for all the generation to finish, we schedule incoming requests to the completed generations. That way, as soon as a generation in a batch is complete, we prefill the batch with the next request. The process looks like Figure 9 .\n\nTransformers supports continuous batching with the generate_batch API. This is not meant for production-grade model serving ‚Äìframeworks like vLLM and SGLang are great at that‚Äì, but can be very helpful for evaluation and experimentation. Here is an example script that runs CB end to end on Qwen/Qwen3-4B-Instruct-2507 .\n\nWe have also performed a benchmark between Continuous Batching and Static Batching with 100 samples. In Figure 9, we note that CB is quite faster than SB.\n\nYou can play around with the benchmark here: SB , CB\n\nWhen you load a large model into your GPU, PyTorch needs to reserve GPU memory for each layer‚Äôs weights . Each of these requests (per layer) takes time, and for multi-billion-parameter models it can mean thousands of tiny memory allocations , adding up to a long wait before the model is ready. Instead of asking the GPU for new memory every single time, it can hold on to a big chunk once and then hand out slices from it quickly.\n\nPyTorch allocators can do exactly this. The catch is that the allocator only gets fast after you‚Äôve given it some memory to work with. If you don‚Äôt ‚Äústock the pantry‚Äù first, you still end up doing many slow trips to the market. This PR (üéâ #36380 ) taught transformers to pre-stock the pantry before it starts copying model weights.\n\nLooks at the device_map (where each layer will live).\n\nPre-allocates a big enough block on each GPU .\n\nThen, as layers are copied in, they just slot neatly into this pre-reserved space.\n\nYou have to make no changes to your existing code, as this is default behaviour in transformers . If you use device_map=\"auto\" or provide your own device map, your model will now load faster automatically. If you‚Äôre running with Tensor Parallel ( tp_plan=\"auto\" ) and torchrun you also benefit from companion changes that make multi-GPU loading smarter.\n\ntransformers moves quickly and it is community-first. The library evolves at the pace of the field because contributors shape it in the open. Pieces added for new models become part of the toolkit and are reused in future integrations.\n\nThis velocity enables day-zero integrations like the GPT-OSS series. As the stack becomes increasingly PyTorch-first , it trims bloat and doubles down on the PyTorch paths that matter in practice. The result is a cleaner core that unlocks new capabilities through community kernels, quantization, and parallelism plans, while also standardizing model definitions so that architectures supported in transformers are a reference and extend across the wider ecosystem.\n\nThis post is a one-time snapshot of a process we repeatedly iterate on towards the same direction: serve the needs of the community. To be up to date with the latest additions to transformers, check the docs and release notes . And please, keep sharing your feedback and releasing your models in transformers for the community to enjoy ü§ó\n\nIf you want to go further into particular topics, here is a list of links that one should visit:\n\nHugging Face GPT-OSS Recipes Repository\n\nWelcome GPT OSS: OpenAI's New Open-Source Model Family\n\nTransformers Documentation: Distributed Inference on Multiple GPUs\n\nMatthew Carrigan's X Thread on GPT OSS Innovations\n\nYouTube Video: OpenAI GPT OSS Announcement\n\nTransformers PR #36380: Faster Model Loading on Accelerators\n\nTransformers PR #36335: Update from_pretrained for Tensor Parallelism\n\nTransformers PR #40039: New Dynamic Sliding Window Layer and Cache\n\nHAN Lab Blog: How Attention Sinks Keep Language Models Stable\n\nMore Articles from our Blog\n\nVery interesting post ‚Äî I appreciate how you highlight MXFP4 quantization and the performance trade-offs in Transformers. In my own experiments, I applied quantization to a mmalam786/distilbert-sst2-int8-onnx-demo and saw ~20-30% latency reduction with negligible accuracy drop. Do you have insights on how MXFP4 handles activation quantization vs weight quantization in edge scenarios? Happy to share my demo if useful: https://www.linkedin.com/posts/dr-mm-alam-93991120b_demofirst-aichips-edgeai-activity-7381674484098883584-0Rwn/?utm_source=share&utm_medium=member_desktop&rcm=ACoAADVZuP0BheDJgKL8dWk-bNo7Yd4zhsOnNL4\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "Jupyter Agents: training LLMs to reason with notebooks",
    "link": "https://huggingface.co/blog/jupyter-agent-2",
    "published_at": "2025-09-10T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "So we built Jupyter Agent to act as an agent that can execute code directly inside a Jupyter notebook and use this environment to solve data analysis and data science tasks. Kaggle notebooks offer a wealth of high quality data analysis notebooks and are made available by Kaggle: Kaggle Notebooks dataset : ~2TB of notebooks. We openly release best-performing checkpoints of tuned Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507 together with the training dataset, which you can try out and experiment with: You can load Jupyter Agent Dataset in just a couple of lines using the following code: You can also use sourced Kaggle datasets directly with E2B code execution using the following code: You use tuned Jupyter Agent Qwen-based models following the Qwen documentation code: For Thinking model you can decode both thinking response and content using the next code: Harder tasks: Generate more challenging, multi-step questions that better reflect real-world analysis.",
    "full_text": "üèÅ Primer: the DABStep Benchmark üéØ First Baseline üîß Primer on Scaffolding üèÉ‚Äç‚ôÇÔ∏è Training Pipeline ‚öôÔ∏è Dataset Pipeline 1. Large-scale deduplication 2. Downloading linked datasets 3. Edu scoring 4. Filtering irrelevant notebooks 5. QA generation 6. Trace generation 7. Final curation üèÉ‚Äç‚ôÇÔ∏è Training Pipeline üìä Results Try Jupyter Agent Yourself üîÆ Next Steps The past year has been all about giving LLMs more tools and autonomy to solve more complex and open ended tasks. The goal of the Jupyter Agent is to give the model the ultimate tool: code execution.\n\nüèÅ Primer: the DABStep Benchmark\n\n‚öôÔ∏è Dataset Pipeline 1. Large-scale deduplication 2. Downloading linked datasets 3. Edu scoring 4. Filtering irrelevant notebooks 5. QA generation 6. Trace generation 7. Final curation\n\nA natural way to display multi-step code execution together with reasoning is within a Jupyter Notebook, which consists of code and markdown cells. So we built Jupyter Agent to act as an agent that can execute code directly inside a Jupyter notebook and use this environment to solve data analysis and data science tasks. Think of it like Cursor , but living natively inside your data science workflow. We built a demo of this vision with Qwen-3 Coder , currently one of the strongest coding models. This is a follow-up to our earlier work on jupyter-agent (v1) .\n\nWhile large models are starting to show useful behavior, the key question is how we can continue improving them. To this end, we focus on strengthening smaller models to perform well on agentic data science tasks as they currently struggle to compete with the large models.\n\nThe goal of this project is to build a pipeline to first generate high-quality training data, then fine-tune an existing small model, and finally evaluate whether the model's performance improves on relevant benchmarks.\n\nLet‚Äôs begin with the last step: selecting a strong benchmark for evaluating models on data science tasks.\n\nIn order to understand if we are making progress towards better data science agents we need a benchmark to measure such capabilities. Last year, in partnership with Adyen , we introduced the DABStep benchmark : a way to evaluate data science agents on realistic tasks. The setup is simple: provide the LLM with datasets and ask it to answer non-trivial data questions.\n\nThis benchmark remains challenging for today‚Äôs LLMs ‚Äî e.g. the best out-of-the-box model is Claude 4 Sonnet which reaches not even 20% accuracy on the hard tasks. You can explore the live leaderboard here .\n\nNow that we identified a good benchmark we can try to climb it! We set out to build a dataset for fine-tuning such that  even a small data agent model could perform well on DABStep.\n\nOur first choice was Qwen3-4B-Thinking-2507 : extremely small (fast to iterate with, easy to run), yet strong enough to act in agentic scenarios.\n\nNot great ‚Äî but a promising starting point, since it left a lot of room for improvement. Let's see how we can improve it!\n\nA core aspect of agents that sets it apart from a pure chat model is the scaffolding built around the model to steer its behaviour. The evaluation script in DABStep for example uses smolagents to execute code. Smolagents comes with predefined behaviors, prompting structures, and expected formats.\n\nWe also studied the Qwen-Agent codebase, where the authors tailoring scaffolding to the model. This makes sense: Claude Code, for example, works shockingly well with Claude Sonnet because their scaffolding is aligned.\n\nSo, we restructured our scaffolding:\n\nStripped it down to ~200 lines of code.\n\nInspired by the spirit of tiny-agents .\n\nüëâ Check it out here: utils.py .\n\nResults: accuracy jumped from 44.4% ‚Üí 59.7% (easy split) . üöÄ\n\nWhile loop with two tools: code execution to run the code and final_answer to return the final answer.\n\nWe differ from Qwen-Agent by explicitly adding a final_answer tool ‚Äî which in our testing has improved performance.\n\nCompared to smolagents, we simplified the scaffolding by removing a lot of prompts and tools. Smolagents also hardcodes a lot of assumptions into the model by using the ReACT framework.\n\nWith simplified scaffolding in place, we focused on fine-tuning Qwen3-4B for data science agentic tasks .\n\nThe recipe to improve a model on a certain task or behaviour is to train it on data that reflects the tasks as closely as possible. A natural starting point is to look at real Jupyter Notebooks and find notebooks that align closely with the task that we plan to tackle, namely data analysis.\n\nKaggle notebooks offer a wealth of high quality data analysis notebooks and are made available by Kaggle:\n\nKaggle Notebooks dataset : ~2TB of notebooks.\n\nKaggle Datasets : 5TB of kaggle datasets that we manually downloaded and linked to the notebooks.\n\nRich metadata for each notebook (authors, datasets used, etc.).\n\nNow that we have good results with a base model it's time to build a dataset that will help us improve it even further. We designed a multi-stage pipeline using Datatrove to clean and prepare Kaggle notebooks at scale.\n\nHere‚Äôs how each step worked:\n\nWe started with ~2TB of Kaggle notebooks and reduced it to ~250GB reusing our work from the BigCode project. As part of the StarCoder2 training data processing the notebooks (without output cells) were already deduplicated.\nMost Kaggle notebooks are small variations or near-identical copies, so this step was essential. Key insight: ~90% of raw notebooks are duplicates, which would have skewed training if left unfiltered.\n\nMost Kaggle notebooks reference external datasets via Kaggle metadata. To make sure the code inside notebooks could actually run, we built a pipeline that automatically fetched these linked datasets. This step was crucial, since many notebooks would otherwise be incomplete or non-executable.\n\nUsing the kagglehub package, we downloaded thousands of datasets ‚Äî about 5TB in total. To keep things manageable and relevant:\n\nWe filtered out datasets containing model checkpoints, large multimodal corpora, or LLM-related files.\n\nWe also excluded very large datasets (10GB+) that couldn‚Äôt fit into the virtual E2B sandboxes we used for execution.\n\nBy the end, we had a rich collection of executable notebooks paired with their datasets, providing the foundation for training agents in realistic, runnable environments.\n\nWe scored notebooks based on educational quality using Qwen3-32B . We saw that using the whole notebook was not optimal, as many contained trivial or broken code. Our educational scoring approach is detailed in edu_scoring.py.\n\nTL;DR: We assigned each notebook a score from 1‚Äì5 based on clarity, completeness, and educational value, and kept only those above a chosen threshold. This filtering removed about 70% of the notebooks.\n\nThis is similar to the insight from the BeyondWeb paper, which showed that using high-quality data is better for synthetic data generation ‚Äî a step we relied on for QA (Question-Answer) generation.\nThis helped the model learn from ‚Äúhigh quality‚Äù notebooks instead of noisy ones.\n\nWe excluded notebooks about training LLMs or unrelated to data analysis.\nWe also removed notebooks that didn‚Äôt actually use datasets through an automated LLM-based filtering process using Qwen3-32B. The implementation of filtering can be found in extract_packages_and_files.py .\n\nTL;DR: We prompted Qwen3-32B to identify and remove notebooks that either (1) had nothing to do with data analysis, or (2) didn‚Äôt actually use datasets. This step removed about 20% of the notebooks.\n\nThis ensured we trained only on relevant data science tasks.\n\nUsing the cleaned notebooks, we generated question‚Äìanswer pairs using Qwen3-32B . The questions and answer are grounded in the real notebook traces so the QA pairs are based on real code execution results. Prompt design: we asked the LLM to produce natural questions that could realistically be asked of the dataset, then validated whether the notebook provided a correct answer.\n\nChallenge: We had to try many prompts to get higher-difficulty questions because LLMs tended to generate trivial ones like \"what is the size of the dataset\". Insight: We broke this into two steps because LLMs tended to hallucinate answers:\n\nGenerate the question and answer.\n\nAsk another LLM (with access to the notebook) to check whether the answer was correct.\n\nThe complete prompting strategy and implementation is available in qa_generation.py .\n\nFinally we want to generate clean code execution traces since even the original notebooks after processing are often open ended and verbose with lots of irrelevant parts. However, we want our Jupyter Agent to get to the result efficiently. To generate cleaner notebook traces for training we generated traces synthetically based on the original notebooks. We have prompted Qwen-3-Coder-480B model to generate a jupyter notebook code to answer the question from the previously generated synthetic QA pair. \nTraces captured step-by-step code execution, including intermediate outputs, which are crucial for agent training.\n\nWe used E2B for our agent to solve the synthetic QA pairs, which required fetching Kaggle datasets so the code could actually run via E2B.\n\nChallenge 1: Many datasets were unavailable. Trick: Since LLMs are strong at code and have a decent world model, we prompted them to act as a code interpreter when the dataset was missing.\n\nChallenge 2: Qwen3-Coder-480B-A35B model does not support thinking mode - how can we extract code commentary? By default it often outputs just a brief comment followed by several steps of code execution. However, we'd like some reasoning or comments between every cell. Trick: When switching from Qwen3-32B to Qwen3-Coder-480B-A35B we noticed that often output message content was empty. This turns out to be a previously known quirk of Qwen3-Coder models in which when using tool calling the model would not return an empty assistant response. We enforce some text commentary through tooling by passing 'comment' as a required field in the code execution tool call. This way when non-reasoning model is used for code cell generation it will by default output some description of its actions from 1st POV, emulating the thinking traces structure.\n\nNote: the generated final answer in the notebook may vary from the answer specified in the QA pair. This is caused by the fact that the agent model could use data preprocessing methods and steps different from the original Kaggle notebook and the synthetic question would not usually specify them. This discrepancy is normal and lays the foundation for a new exciting research direction of how language models tend to treat data analysis and whether they do it differently from humans. For full transparency we keep both LLM-generated final answer and original answer from the real Kaggle notebook as a signal of model's performance. We encourage the community to try different dataset mixes to see how they can push performance even further.\n\nWe truncated overly long outputs and filtered out trivial traces to prevent content length issues and keep only high-quality traces. We kept non-trivial, multi-turn traces aligned with DABStep-style tasks. The resulting Jupyter Agent Dataset became the foundation for SFT on Qwen3-4B models with 51k synthetic notebooks and almost 0.2B tokens.\n\nWith this dataset in hand, the natural next step is to see whether it actually helps our model become a stronger data science agent. Let‚Äôs move on to the training pipeline and evaluate the impact!\n\nWith the curated dataset ready, we turned to the key question: does this data actually help the model get better at solving data analysis tasks? To find out, we set up a simple fine-tuning pipeline and ran experiments to measure the impact of training on our synthetic notebooks.\n\nSome training steps turned out to be particularly interesting and gave us useful insights:\n\nFor trace generation, we used LLMs to generate QA pairs, which gave us a verifiable environment .\n\nFinally, we fine-tuned Qwen3-4B with TRL . Used assistant_loss_only=True ‚Üí small performance boost. Added neftune noise for full-parameter multi-epoch training ‚Üí avoids overfitting.\n\nUsed assistant_loss_only=True ‚Üí small performance boost.\n\nAdded neftune noise for full-parameter multi-epoch training ‚Üí avoids overfitting.\n\nPrompting models for tool calling is tricky: not all prompts deliver the same performance ( Qwen docs ).\n\nWe had to manually test each one to find what worked best.\n\nThere‚Äôs no standardization in response formats for tool calling, making it difficult to switch between models.\n\nNative Qwen's generation prompt is not adapted to assistant_loss_only=True training mode in TRL which requires to have generation tokens by default. Thus, we adapt the original chat templates by wrapping the assistant response part in the generation tags.\n\nTraining thinking models on short reasoning texts may disrupt model capabilities ‚Üí full-parameter training works better comparing to PEFT in this case.\n\nOur complete training implementation, including hyperparameter configurations and template adaptations, is available in our finetuning directory in our repo.\n\nFirst, we generated our final dataset using Qwen3-Coder-480B-A35B which contains high quality code and short reasoning-like traces. Afterwards, we started our training and we have experimented with various configurations like PEFT/adapters vs. full-parameter tuning, learning rate, number of epochs, adding noise and others. We found out, that full-parameter fine-tuning allows the model to learn and replicate the Qwen3-Coder-480B-A35B behavior response quality better with shorter supporting commentary fitting more to the data analysis task without unnecessary long reasoning.\n\nWe have done a small ablation study on the impact of no. training epochs:\n\nWe observe that it is beneficial to have a bit more epochs than usual for SFT with lower learning rate and higher neftune noise (7). Finally, we compare our trained models with implemented scaffolding to define the pure impact of our training dataset. In summary, we can see up to 36%/22% boost on DABStep easy score compared with base/scaffolded model:\n\nWe can also see, that the hard score can increase too even though our dataset is focused on easier questions:\n\nFrom figures above one can notice a noticeable impact of both new scaffolding and tuning on our synthetic notebooks. This makes Qwen-4B (with our pipeline + scaffolding) a state-of-the-art small-model agent on DABStep.\n\nIn practice, the model can now solve a wide range of realistic Kaggle-style data analysis tasks with consistent execution. It‚Äôs not yet strong enough for the hardest queries, but we‚Äôve shown that even small models can become powerful agents when paired with the right data and scaffolding.\n\nThese results demonstrate that even small models can become powerful data science agents with the right training approach. Ready to try it yourself? We've made everything openly available so you can experiment with our fine-tuned models and dataset.\n\nWe openly release best-performing checkpoints of tuned Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507 together with the training dataset, which you can try out and experiment with:\n\nYou can load Jupyter Agent Dataset in just a couple of lines using the following code:\n\nYou can also use sourced Kaggle datasets directly with E2B code execution using the following code:\n\nYou use tuned Jupyter Agent Qwen-based models following the Qwen documentation code:\n\nFor Thinking model you can decode both thinking response and content using the next code:\n\nHarder tasks: Generate more challenging, multi-step questions that better reflect real-world analysis.\n\nScaling up: Train on larger volumes of curated traces to push beyond the current 3.4% performance on the hard split.\n\nDistillation: Investigate knowledge distillation, which has shown strong results for improving small models.\n\nReinforcement Learning (RL): Build an RL environment, which has been shown to achieve state-of-the-art performance on agentic tasks. Since our QA setup already provides a verifiable environment, we could leverage it directly for RL training.\n\nMaybe this will lead to‚Ä¶ Jupyter-Agent 3. üòâ\n\nWe hope that our findings will inspire others to continue progress in developing more powerful notebook coding agents and we're excited to see what the community builds next. Dive into our jupyter-agent dataset on the ü§ó Hub and explore the codebase at https://github.com/huggingface/jupyter-agent to start your own experiments on agents for jupyter notebooks.\n\nMore Articles from our Blog\n\n¬∑ Sign up or log in to comment"
  },
  {
    "title": "MCP for Research: How to Connect AI to Research Tools",
    "link": "https://huggingface.co/blog/mcp-for-research",
    "published_at": "2025-08-18T00:00:00+00:00",
    "source": "Hugging Face - Blog",
    "tags": [],
    "summary": "MCP Integration Setup and Usage Quick Setup Learn More Academic research involves frequent research discovery : finding papers, code, related models and datasets. This comes with the same caveats as scripting: Faster than manual research, but error-prone without human guidance Quality depends on the implementation Understanding the lower layers (both manual and scripted) leads to better implementations The easiest way to add the Research Tracker MCP is through Hugging Face MCP Settings : Search for \"research-tracker-mcp\" in the available tools Click to add it to your tools Follow the provided setup instructions for your specific client (Claude Desktop, Cursor, Claude Code, VS Code, etc.) This workflow leverages the Hugging Face MCP server, which is the standard way to use Hugging Face Spaces as MCP tools. Hugging Face MCP Course - Complete guide from basics to building your own tools MCP Official Documentation - Protocol specifications and architecture Gradio MCP Guide - Turn Python functions into MCP tools Building the Hugging Face MCP Server - Production implementation case study Hugging Face Discord - MCP development discussions Ready to automate your research discovery?",
    "full_text": "Research Discovery: Three Layers of Abstraction 1. Manual Research 2. Scripted Tools 3. MCP Integration Setup and Usage Quick Setup Learn More Academic research involves frequent research discovery : finding papers, code, related models and datasets. This typically means switching between platforms like arXiv , GitHub , and Hugging Face , manually piecing together connections.\n\nResearch Discovery: Three Layers of Abstraction 1. Manual Research 2. Scripted Tools 3. MCP Integration\n\nSetup and Usage Quick Setup\n\nThe Model Context Protocol (MCP) is a standard that allows agentic models to communicate with external tools and data sources. For research discovery, this means AI can use research tools through natural language requests, automating platform switching and cross-referencing.\n\nMuch like software development, research discovery can be framed in terms of layers of abstraction.\n\nAt the lowest level of abstraction, researchers search manually and cross-reference by hand.\n\nThis manual approach becomes inefficient when tracking multiple research threads or conducting systematic literature reviews. The repetitive nature of searching across platforms, extracting metadata, and cross-referencing information naturally leads to automation through scripting.\n\nPython scripts automate research discovery by handling web requests, parsing responses, and organizing results.\n\nThe research tracker demonstrates systematic research discovery built from these types of scripts.\n\nWhile scripts are faster than manual research, they often fail to automatically collect data due to changing APIs, rate limits, or parsing errors. Without human oversight, scripts may miss relevant results or return incomplete information.\n\nMCP makes these same Python tools accessible to AI systems through natural language.\n\nThe AI orchestrates multiple tools, fills information gaps, and reasons about results:\n\nThis can be viewed as an additional layer of abstraction above scripting, where the \"programming language\" is natural language. This follows the Software 3.0 Analogy , where the natural language research direction is the software implementation.\n\nThis comes with the same caveats as scripting:\n\nFaster than manual research, but error-prone without human guidance\n\nQuality depends on the implementation\n\nUnderstanding the lower layers (both manual and scripted) leads to better implementations\n\nThe easiest way to add the Research Tracker MCP is through Hugging Face MCP Settings :\n\nSearch for \"research-tracker-mcp\" in the available tools\n\nClick to add it to your tools\n\nFollow the provided setup instructions for your specific client (Claude Desktop, Cursor, Claude Code, VS Code, etc.)\n\nThis workflow leverages the Hugging Face MCP server, which is the standard way to use Hugging Face Spaces as MCP tools. The settings page provides client-specific configuration that's automatically generated and always up-to-date.\n\nHugging Face MCP Course - Complete guide from basics to building your own tools\n\nMCP Official Documentation - Protocol specifications and architecture\n\nGradio MCP Guide - Turn Python functions into MCP tools\n\nBuilding the Hugging Face MCP Server - Production implementation case study\n\nHugging Face Discord - MCP development discussions\n\nReady to automate your research discovery? Try the Research Tracker MCP or build your own research tools with the resources above.\n\nMore Articles from our Blog\n\nI might be biased: https://github.com/mixelpixx/Google-Search-MCP-Server\n\n¬∑ Sign up or log in to comment"
  }
]
